diff --git a/src/omnipkg/isolation/patchers.py b/src/omnipkg/isolation/patchers.py
index 4498f48..6cc6ec7 100644
--- a/src/omnipkg/isolation/patchers.py
+++ b/src/omnipkg/isolation/patchers.py
@@ -1,24 +1,32 @@
-# omnipkg/tf_smart_patcher.py - FIXED CIRCULAR IMPORT HANDLING
+# omnipkg/tf_smart_patcher.py - LAZY LOADING WITH GRACEFUL FALLBACKS
 import sys
 import importlib
 import builtins
-import threading
 import warnings
+import threading
 from types import ModuleType
+
 try:
     from omnipkg.common_utils import ProcessCorruptedException
 except ImportError:
     class ProcessCorruptedException(Exception): pass
+
 try:
     from .common_utils import safe_print
 except ImportError:
-    from omnipkg.common_utils import safe_print
+    try:
+        from omnipkg.common_utils import safe_print
+    except ImportError:
+        # Ultimate fallback if safe_print isn't available
+        def safe_print(msg):
+            try:
+                print(msg)
+            except:
+                pass
 
 _tf_smart_initialized = False
 _tf_module_cache = {}
 _original_import_func = builtins.__import__
-
-# Track circular import healing stats
 _circular_import_stats = {}
 
 _tf_circular_deps_known = {
@@ -29,11 +37,52 @@ _tf_circular_deps_known = {
     'compat': 'tensorflow.python.util.compat',
     'dispatch': 'tensorflow.python.util.dispatch',
 }
+_recursion_guard = threading.local()
 
+def _patch_numpy_for_tf_recursion():
+    """
+    Applies a patch to numpy.issubdtype to prevent infinite recursion
+    when used with certain versions of TensorFlow.
+    """
+    try:
+        import numpy.core.numerictypes as nt
+        
+        # Check if we've already patched it
+        if hasattr(nt.issubdtype, '__omnipkg_healed__'):
+            return
+
+        original_issubdtype = nt.issubdtype
+        
+        def healed_issubdtype(arg1, arg2):
+            """
+            A wrapper around the original issubdtype that prevents re-entry.
+            """
+            if getattr(_recursion_guard, 'in_issubdtype_check', False):
+                # We are already in this function, so we're in a recursive loop.
+                # Break the cycle by returning a safe default.
+                return False
+            
+            _recursion_guard.in_issubdtype_check = True
+            try:
+                # Call the original, real function
+                return original_issubdtype(arg1, arg2)
+            finally:
+                # Always release the guard
+                _recursion_guard.in_issubdtype_check = False
+
+        healed_issubdtype.__omnipkg_healed__ = True
+        nt.issubdtype = healed_issubdtype
+        safe_print("ğŸ©¹ [OMNIPKG] Healed NumPy's issubdtype for TensorFlow.")
+
+    except (ImportError, AttributeError) as e:
+        # This might happen if NumPy isn't installed or has an unusual structure.
+        # It's safe to ignore and proceed without the patch.
+        safe_print(f"âš ï¸  [OMNIPKG] Could not apply NumPy recursion patch: {e}")
 
 def smart_tf_patcher():
     """
-    UNIFIED patcher for TensorFlow and PyTorch with C++ reality stabilization.
+    Install a lazy import hook that only activates TF/PyTorch/NumPy handling
+    when those packages are actually being imported.
     """
     global _tf_smart_initialized
 
@@ -43,18 +92,54 @@ def smart_tf_patcher():
     if _tf_smart_initialized:
         return
 
-    safe_print("ğŸ§  [OMNIPKG] Installing import hooks with C++ stabilization")
-
-    _install_cpp_reality_anchors()
-
     def genius_import(name, globals=None, locals=None, fromlist=(), level=0):
         """
-        A single import hook that understands both TensorFlow and PyTorch.
+        Lazy import hook that only handles TF/PyTorch/NumPy when encountered.
+        NOW WITH STANDARD LIBRARY SAFEGUARD to prevent interference with packaging operations.
         """
+        
+        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+        # CRITICAL SAFEGUARD: Never intercept standard library imports
+        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+        STDLIB_WHITELIST = {
+            'importlib', 'importlib.metadata', 'importlib_metadata',
+            'pkg_resources', 'setuptools', 'pip', 'distutils',
+            '_distutils_hack', 'packaging', 'wheel'
+        }
+        
+        # Check if this is a standard library import that we should ignore
+        if name:
+            # Direct match or submodule of whitelisted packages
+            if name in STDLIB_WHITELIST or any(name.startswith(f"{pkg}.") for pkg in STDLIB_WHITELIST):
+                return _original_import_func(name, globals, locals, fromlist, level)
+        
+        # Also check fromlist for whitelisted packages
+        if fromlist:
+            for item in fromlist:
+                item_str = str(item)
+                if any(pkg in item_str for pkg in STDLIB_WHITELIST):
+                    return _original_import_func(name, globals, locals, fromlist, level)
+        
+        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+        # Continue with existing special handling logic
+        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+        is_torch_or_numpy = name and ('torch' in name or 'numpy' in name)
+        is_tf_import = name and (name == 'tensorflow' or name.startswith('tensorflow.'))
+        is_tf_submodule = fromlist and any('tensorflow' in str(f) for f in fromlist) if fromlist else False
+        
+        needs_special_handling = is_torch_or_numpy or is_tf_import or is_tf_submodule
+        
+        if not needs_special_handling:
+            return _original_import_func(name, globals, locals, fromlist, level)
+
+        # Patch numpy BEFORE TensorFlow imports it
+        if is_tf_import and 'tensorflow' not in sys.modules:
+            _patch_numpy_for_tf_recursion()
+
         # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
         # torch/numpy SPECIFIC LOGIC (Warning Suppression)
         # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-        if 'torch' in name or 'numpy' in name:
+        if is_torch_or_numpy:
             with warnings.catch_warnings():
                 warnings.filterwarnings(
                     "ignore",
@@ -66,15 +151,13 @@ def smart_tf_patcher():
                     message="A module that was compiled using NumPy 1.x cannot be run in NumPy 2.+",
                     category=UserWarning
                 )
-                pass
+                return _original_import_func(name, globals, locals, fromlist, level)
 
         # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
         # tensorflow SPECIFIC LOGIC (Reload Protection & Circular Deps)
         # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-        is_tf_import = name and (name == 'tensorflow' or name.startswith('tensorflow.'))
-        is_tf_submodule = fromlist and any('tensorflow' in str(f) for f in fromlist) if fromlist else False
-
         if is_tf_import:
+            # Only check for reload if TensorFlow was previously loaded
             if hasattr(builtins, '__omnipkg_tf_loaded_once__') and 'tensorflow' not in sys.modules:
                 safe_print("â˜¢ï¸  [OMNIPKG] FATAL TENSORFLOW RELOAD DETECTED!")
                 raise ProcessCorruptedException(
@@ -86,13 +169,19 @@ def smart_tf_patcher():
             if _detect_circular_import_scenario(name, fromlist, globals):
                 return _handle_circular_import(name, fromlist, globals)
             
-            # Handle partial initialization scenarios
-            if _is_partially_initialized_tf(globals):
-                return _handle_partial_initialization(name, fromlist, globals)
+            # Handle partial initialization scenarios (only if numpy is available)
+            try:
+                if _is_partially_initialized_tf(globals):
+                    return _handle_partial_initialization(name, fromlist, globals)
+            except Exception:
+                pass  # If checks fail, proceed with normal import
             
-            # Handle C++/Python boundary crossing
-            if _is_cpp_boundary_import(name, fromlist):
-                return _handle_cpp_boundary_import(name, fromlist, globals)
+            # Handle C++/Python boundary crossing (only if needed)
+            try:
+                if _is_cpp_boundary_import(name, fromlist):
+                    return _handle_cpp_boundary_import(name, fromlist, globals)
+            except Exception:
+                pass  # If C++ handling fails, proceed with normal import
 
         # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
         # THE ACTUAL IMPORT
@@ -102,6 +191,7 @@ def smart_tf_patcher():
         if is_tf_import and module:
             if not hasattr(builtins, '__omnipkg_tf_loaded_once__'):
                 builtins.__omnipkg_tf_loaded_once__ = True
+            _patch_numpy_for_tf_recursion()
 
         return module
 
@@ -109,18 +199,21 @@ def smart_tf_patcher():
     builtins.__import__ = genius_import
     _tf_smart_initialized = True
 
-def _install_cpp_reality_anchors():
-    """Install handles for TensorFlow's C++ psyche to grip during reality shifts."""
-    
-    # Create stable memory anchors for C++ extensions
-    _tf_module_cache['cpp_reality_anchor'] = {
-        'numpy_dtype_mappings': _create_stable_dtype_mappings(),
-        'memory_layout_guides': _create_memory_layout_guides(),
-        'type_conversion_handles': _create_type_conversion_handles(),
-    }
+# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+# LAZY C++ STABILIZATION (only when actually needed)
+# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+
+def _lazy_init_cpp_reality_anchors():
+    """Lazily initialize C++ reality anchors only when first needed."""
+    if 'cpp_reality_anchor' not in _tf_module_cache:
+        _tf_module_cache['cpp_reality_anchor'] = {
+            'numpy_dtype_mappings': _create_stable_dtype_mappings(),
+            'memory_layout_guides': _create_memory_layout_guides(),
+            'type_conversion_handles': _create_type_conversion_handles(),
+        }
 
 def _create_stable_dtype_mappings():
-    """Create stable dtype mappings so C++ doesn't get confused during version switches."""
+    """Create stable dtype mappings (only if numpy is available)."""
     stable_mappings = {}
     try:
         import numpy as np
@@ -132,7 +225,7 @@ def _create_stable_dtype_mappings():
             'bool': np.bool_,
         }
     except ImportError:
-        pass
+        pass  # NumPy not available, return empty mappings
     return stable_mappings
 
 def _create_memory_layout_guides():
@@ -168,27 +261,40 @@ def _is_cpp_boundary_import(name, fromlist):
     return False
 
 def _handle_cpp_boundary_import(name, fromlist, globals):
-    """Handle C++/Python boundary imports."""
-    _stabilize_cpp_psyche()
+    """Handle C++/Python boundary imports with lazy initialization."""
+    try:
+        _lazy_init_cpp_reality_anchors()  # Only init if not already done
+        _stabilize_cpp_psyche()
+    except Exception:
+        pass  # If stabilization fails, continue anyway
+    
     result = _original_import_func(name, globals, None, fromlist, level=0)
-    _post_import_cpp_stabilization(name, result)
+    
+    try:
+        _post_import_cpp_stabilization(name, result)
+    except Exception:
+        pass  # If post-stabilization fails, continue anyway
     
     return result
 
 def _stabilize_cpp_psyche():
-    """Stabilize TensorFlow's C++ extensions before they freak out."""
+    """Stabilize TensorFlow's C++ extensions (only if TF is loaded)."""
     if 'tensorflow' in sys.modules:
         tf_module = sys.modules['tensorflow']
         if not hasattr(tf_module, '__omnipkg_reality_anchors__'):
-            tf_module.__omnipkg_reality_anchors__ = _tf_module_cache['cpp_reality_anchor']
+            if 'cpp_reality_anchor' in _tf_module_cache:
+                tf_module.__omnipkg_reality_anchors__ = _tf_module_cache['cpp_reality_anchor']
 
 def _post_import_cpp_stabilization(name, module):
     """Apply post-import stabilization to C++ modules."""
-    if hasattr(module, '__file__') and '.so' in str(module.__file__):
-        module.__omnipkg_cpp_stabilized__ = True
+    try:
+        if hasattr(module, '__file__') and module.__file__ and '.so' in str(module.__file__):
+            module.__omnipkg_cpp_stabilized__ = True
+    except Exception:
+        pass
 
 def _tensor_to_numpy_stabilized(tensor):
-    """Stabilized tensor to numpy conversion with reality anchors."""
+    """Stabilized tensor to numpy conversion (with graceful fallback)."""
     try:
         if hasattr(tensor, 'numpy'):
             result = tensor.numpy()
@@ -196,18 +302,20 @@ def _tensor_to_numpy_stabilized(tensor):
                 result.flags.writeable = True
             return result
     except Exception:
-        return None
+        pass
+    return None
 
 def _numpy_to_tensor_stabilized(array):
-    """Stabilized numpy to tensor conversion."""
+    """Stabilized numpy to tensor conversion (with graceful fallback)."""
     try:
         stabilized_array = _stabilize_numpy_array(array)
         return stabilized_array
     except Exception:
-        return array
+        pass
+    return array
 
 def _stabilize_numpy_array(array):
-    """Ensure numpy array is in a stable state for C++ consumption."""
+    """Ensure numpy array is in a stable state (only if numpy is available)."""
     try:
         import numpy as np
         if not array.flags['C_CONTIGUOUS']:
@@ -216,7 +324,12 @@ def _stabilize_numpy_array(array):
             array = array.copy()
         return array
     except Exception:
-        return array
+        pass
+    return array
+
+# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+# CIRCULAR IMPORT HANDLING (TensorFlow-specific)
+# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 
 def _detect_circular_import_scenario(name, fromlist, globals):
     """Detect if we're in a circular import scenario."""
@@ -236,62 +349,45 @@ def _detect_circular_import_scenario(name, fromlist, globals):
     
     for pattern_module, pattern_import in circular_patterns:
         if pattern_module in current_module and fromlist and pattern_import in fromlist:
-            # Track stats instead of printing each one
             _circular_import_stats[pattern_import] = _circular_import_stats.get(pattern_import, 0) + 1
             return True
     
     return False
 
 def _handle_circular_import(name, fromlist, globals):
-    """
-    FIXED: Intelligently handle circular imports by directly importing and 
-    injecting into sys.modules, then returning the parent module.
-    """
+    """Handle circular imports by pre-loading dependencies."""
     if not fromlist:
         return _original_import_func(name, globals, None, fromlist, level=0)
     
-    # Import each circular dependency directly and cache it
     successfully_imported = {}
     for import_name in fromlist:
         if import_name in _tf_circular_deps_known:
             real_module_path = _tf_circular_deps_known[import_name]
             
-            # Check if already imported
             if real_module_path in sys.modules:
                 successfully_imported[import_name] = sys.modules[real_module_path]
             else:
                 try:
                     target_module = importlib.import_module(real_module_path)
-                    # Ensure it's in sys.modules
                     sys.modules[real_module_path] = target_module
                     successfully_imported[import_name] = target_module
                 except ImportError:
                     pass
     
-    # If we couldn't import the dependencies, don't try to proceed
     if not successfully_imported:
-        # Fall back to original import and let it fail naturally
         return _original_import_func(name, globals, None, fromlist, level=0)
     
-    # Now do the original import - the circular deps are already loaded
     try:
         result = _original_import_func(name, globals, None, fromlist, level=0)
         return result
     except ImportError:
-        # If the original import still fails, we have two options:
-        # 1. If the parent module exists, return it with injected attributes
-        # 2. Otherwise, return a namespace with just the imported items
-        
         if name in sys.modules:
-            # Parent module exists, inject the successfully imported items
             parent = sys.modules[name]
             for import_name, module in successfully_imported.items():
                 if not hasattr(parent, import_name):
                     setattr(parent, import_name, module)
             return parent
         
-        # Create a minimal namespace object (not a full module)
-        # This is safer than creating a synthetic module
         class CircularImportNamespace:
             def __init__(self, items):
                 for key, value in items.items():
@@ -300,7 +396,7 @@ def _handle_circular_import(name, fromlist, globals):
         return CircularImportNamespace(successfully_imported)
 
 def print_circular_import_summary():
-    """Print a summary of circular imports healed (call at end of loading)."""
+    """Print a summary of circular imports healed."""
     if _circular_import_stats:
         summary = ", ".join(f"{dep}Ã—{count}" for dep, count in sorted(_circular_import_stats.items()))
         safe_print(f"ğŸ”„ [OMNIPKG] Healed circular imports: {summary}")
@@ -325,7 +421,10 @@ def _handle_partial_initialization(name, fromlist, globals):
     """Handle partial initialization."""
     parent_module = globals['__name__'] if globals else name
     if parent_module in sys.modules:
-        _force_complete_initialization(parent_module)
+        try:
+            _force_complete_initialization(parent_module)
+        except Exception:
+            pass  # If force initialization fails, continue anyway
     
     return _original_import_func(name, globals, None, fromlist, level=0)
 
