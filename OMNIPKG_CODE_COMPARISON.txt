=========================================================================================
 OMNIPKG CODE COMPARISON
=========================================================================================
 LEFT SIDE:  The current, clean code (GOOD)
 RIGHT SIDE: Your backed-up, modified code (FEATURE)

HOW TO READ THIS:
 - Lines with a '|' in the middle have been changed.
 - Lines with a '<' on the left were removed from your version.
 - Lines with a '>' on the right were added in your version.
 - Identical lines are hidden to make the file shorter.
-----------------------------------------------------------------------------------------

=========================================================================================
START OF: omnipkg/core.py
=========================================================================================

def _get_core_dependencies() -> set:									     |	# In omnipkg/core.py (top-level function)
													     >
													     >	def _get_core_dependencies() -> list:
    Correctly reads omnipkg's own production dependencies and returns them as a set.			     |	    Correctly reads omnipkg's own production dependencies from pyproject.toml
													     >	    and returns them as a list of full requirement strings, respecting
													     >	    version specifiers and environment markers.
													     >	        # First, try the installed package metadata (most reliable for users)
        reqs = pkg_meta.get_all('Requires-Dist') or []							     |	        # This correctly returns the full strings like "safety>=3.0; python_version >= '3.10'"
        return {canonicalize_name(re.match('^[a-zA-Z0-9\\-_.]+', req).group(0)) for req in reqs if re.match( |	        return pkg_meta.get_all('Requires-Dist') or []
													     >	        # Fallback for developer/source installations
                return pyproject_data['project'].get('dependencies', [])				     |	                
													     >	                # This returns the list of strings directly from the toml file
													     >	                dependencies = pyproject_data['project'].get('dependencies', [])
													     >	                
													     >	                # Also include 'full' optional dependencies during bootstrap
													     >	                # to ensure tools like tqdm and redis are available.
													     >	                optional_deps = pyproject_data['project'].get('optional-dependencies', {})
													     >	                dependencies.extend(optional_deps.get('full', []))
													     >	                
													     >	                return dependencies
        # --- START OF MODIFICATIONS ---								     |	        # --- THIS IS THE FINAL, CORRECT INITIALIZATION SEQUENCE ---
        # STEP 1: Establish the environment's unique identity using the robust new method.		     |	        self.venv_path = self._get_venv_root()
        self.venv_path = self._get_venv_root() # <-- USE THE NEW, STABLE METHOD				     |	        
													     >	        # The env_id is now reliably calculated from the one true venv_path.
													     >	        self.env_id = hashlib.md5(str(self.venv_path.resolve()).encode()).hexdigest()[:8]
													     >	        
													     >	        # Override is still respected for subprocess communication.
													     <
        else:												     |	        
            self.env_id = hashlib.md5(str(self.venv_path.resolve()).encode()).hexdigest()[:8]		     <
													     <
        # STEP 2: Initialize basic paths using the new CI-aware method.					     <
        self._preferred_version = (3, 11)								     |	        self.config_path = self._get_config_path()
        self.config_path = self._get_config_path() # <-- USE THE NEW, STABLE METHOD			     <
        # --- END OF MODIFICATIONS ---									     |	        
													     <
        # The rest of your __init__ method remains unchanged.						     <
        # After this point, self.config is guaranteed to be loaded.					     |	        
            # This is a critical failure state.								     <
            # Stop initialization if config loading failed.						     <
													     >	        # --- END OF SEQUENCE ---
        # STEP 4: Perform the one-time ENVIRONMENT setup (e.g., installing Python 3.11).		     |	    def _get_venv_root(self) -> Path:
        # This is separate from the config file setup and only runs once per venv.			     |	        """
        is_nested_interpreter = '.omnipkg/interpreters' in str(Path(sys.executable).resolve())		     |	        DEFINITIVE FIX: Finds the true virtual environment root with a robust,
        setup_complete_flag = self.venv_path / '.omnipkg' / '.setup_complete'				     |	        prioritized search that correctly handles nested conda environments and
													     >	        omnipkg-managed interpreters.
													     >	        """
													     >	        current_executable = Path(sys.executable).resolve()
        if not setup_complete_flag.exists() and not is_nested_interpreter:				     |	        # PRIORITY 1: If we are inside an omnipkg-managed interpreter, the true
            if not suppress_init_messages:								     |	        # venv root is the directory that CONTAINS the '.omnipkg' directory.
                print('\n' + '=' * 60)									     |	        # This is the most reliable method and stops contamination.
                print(_('  üöÄ OMNIPKG ONE-TIME ENVIRONMENT SETUP'))					     |	        try:
                print('=' * 60)										     |	            parts = current_executable.parts
            												     |	            if '.omnipkg' in parts and 'interpreters' in parts:
            try:											     |	                omnipkg_index = parts.index('.omnipkg')
                # We can now safely call other omnipkg components because our own config is loaded.	     |	                # The venv_path is the parent of the '.omnipkg' directory.
                if not suppress_init_messages:								     |	                return Path(*parts[:omnipkg_index])
                    print(_('   - Step 1: Registering the native Python interpreter...'))		     |	        except (ValueError, IndexError):
                native_version_str = f'{sys.version_info.major}.{sys.version_info.minor}'		     |	            pass
                self._register_and_link_existing_interpreter(Path(sys.executable), native_version_str)	     |
													     |	        # PRIORITY 2: Check for a pyvenv.cfg file by walking up from the executable.
                if sys.version_info[:2] != self._preferred_version:					     |	        # This is the standard way to identify a venv.
                    if not suppress_init_messages:							     |	        search_dir = current_executable.parent
                        print(_('\n   - Step 2: Setting up the required Python 3.11 control plane...'))	     |	        while search_dir != search_dir.parent:
                    											     |	            if (search_dir / 'pyvenv.cfg').exists():
                    # Temporarily create an omnipkg core instance to access its methods.		     |	                return search_dir
                    temp_omnipkg = omnipkg(config_manager=self)						     |	            search_dir = search_dir.parent
                    result_code = temp_omnipkg._fallback_to_download('3.11')				     |
                    if result_code != 0:								     |	        # PRIORITY 3: Use CONDA_PREFIX if it's set and we are running inside it.
                        raise RuntimeError('Failed to set up the Python 3.11 control plane.')		     |	        # This is crucial for conda environments.
													     |	        conda_prefix = os.environ.get('CONDA_PREFIX')
                setup_complete_flag.parent.mkdir(parents=True, exist_ok=True)				     |	        if conda_prefix:
                setup_complete_flag.touch()								     |	            conda_path = Path(conda_prefix).resolve()
                if not suppress_init_messages:								     |	            if str(current_executable).startswith(str(conda_path)):
                    print('\n' + '=' * 60)								     |	                return conda_path
                    print(_('  ‚úÖ SETUP COMPLETE'))							     |
                    print('=' * 60)									     |	        # PRIORITY 4: Fallback to VIRTUAL_ENV.
                    print(_('Your environment is now fully managed by omnipkg.'))			     |	        venv_env = os.environ.get('VIRTUAL_ENV')
                    print('=' * 60)									     |	        if venv_env:
            except Exception as e:									     |	            return Path(venv_env).resolve()
                if not suppress_init_messages:								     |
                    print(_('‚ùå A critical error occurred during one-time setup: {}').format(e))	     |	        # FINAL FALLBACK: If all else fails, use sys.prefix.
                    import traceback									     |	        return Path(sys.prefix)
                    traceback.print_exc()								     <
                if setup_complete_flag.exists():							     <
                    setup_complete_flag.unlink(missing_ok=True)						     <
                sys.exit(1)										     <
        """												     |	        # This function is now correct and does not need changes.
        Sets an environment-specific flag indicating that a new interpreter				     <
        needs its knowledge base built.									     <
        """												     <
        # --- THE FIX ---										     <
        # The flag is now named with the environment ID to make it unique.				     <
        # --- END FIX ---										     <
													     <
													     |	        with FileLock(str(lock_file)):
        lock_file = self.venv_path / '.omnipkg' / '.needs_kb_rebuild.lock'				     <
        flag_file.parent.mkdir(parents=True, exist_ok=True)						     <
        												     <
        with FileLock(lock_file):									     <
                        versions_to_rebuild = json.load(f)						     |	                        content = f.read()
                except (json.JSONDecodeError, IOError):							     |	                        if content: versions_to_rebuild = json.loads(content)
                    pass # Overwrite corrupted file							     |	                except (json.JSONDecodeError, IOError): pass
            												     <
            												     <
        """												     |	        # This function is now correct and does not need changes.
        Determines the path to the config file.								     <
        Prioritizes the OMNIPKG_CONFIG_PATH environment variable for CI/automation.			     <
        """												     <
													     |	        config_dir = self.venv_path / '.omnipkg_config'
        # Fallback for local use									     <
        config_dir = Path.home() / '.config' / 'omnipkg'						     <
													     >	    # In core.py, inside the ConfigManager class
													     >
        Finds the virtual environment root with enhanced validation.					     |	        Finds the virtual environment root. CRITICAL FIX: If running inside a
        THIS IS THE UNIFICATION FIX: It prioritizes finding the true venv root				     |	        nested .omnipkg interpreter, resolve up to the true VENV root.
        so that all interpreters within it share the same env_id.					     |	        """
        """												     <
        # PRIORITY 1: An override from a relaunch is the absolute source of truth.			     <
        override = os.environ.get('OMNIPKG_VENV_ROOT')							     <
        if override:											     <
            return Path(override)									     <
													     <
        # PRIORITY 2: The definitive pyvenv.cfg search. This is the most reliable method.		     |	        # If the executable path contains the managed interpreters subdirectory,
        # It correctly finds the root even when running from a nested managed interpreter.		     |	        # we know the true VENV root is upstream (e.g., two or three levels up).
        search_dir = current_executable.parent								     |	        # This prevents the nested interpreter path from becoming the new "prefix".
        while search_dir != search_dir.parent:  # Stop at the filesystem root				     |	        if '.omnipkg/interpreters' in str(current_executable):
            if (search_dir / 'pyvenv.cfg').exists():							     |	            # We assume the structure is VENV_ROOT/.omnipkg/interpreters/cpython-X.X/...
                return search_dir									     |	            # The safest approach is to search up from the executable path for the .omnipkg folder.
            search_dir = search_dir.parent								     |	            search_path = current_executable
													     >	            while search_path != search_path.parent:
													     >	                if search_path.name == '.omnipkg':
													     >	                    # Return the parent of .omnipkg, which is the VENV_ROOT.
													     >	                    return search_path.parent.resolve()
													     >	                search_path = search_path.parent
        # PRIORITY 3: VIRTUAL_ENV, but ONLY if we are currently running inside it.			     |	        # Standard VENV detection logic (fallback)
        venv_path_str = os.environ.get('VIRTUAL_ENV')							     |	        venv_path_str = os.environ.get('VIRTUAL_ENV') or os.environ.get('CONDA_PREFIX')
        # FINAL FALLBACK: If all else fails, use sys.prefix. This is now the last resort.		     |	        # Final fallback
													     |	        
            print(_('   üî© Running: {}').format(' '.join(cmd)))						     |	            print(_(' üî© Running: {}').format(' '.join(cmd)))
                print(_('   ‚ùå {}').format(error_msg))							     |	                print(_(' ‚ùå {}').format(error_msg))
                print('   --- Stderr ---'); print(e.stderr); print('   ----------------')		     |	                print(' --- Stderr ---'); print(e.stderr); print(' ----------------')
													     >	        def get_python_version(python_exe: Path) -> tuple:
													     >	            """Get the Python version as a tuple (major, minor)."""
													     >	            try:
													     >	                result = subprocess.run(
													     >	                    [str(python_exe), '-c', 'import sys; print(f"{sys.version_info.major}.{sys.version_info.
													     >	                    capture_output=True, text=True, check=True
													     >	                )
													     >	                version_str = result.stdout.strip()
													     >	                major, minor = map(int, version_str.split('.'))
													     >	                return (major, minor)
													     >	            except Exception:
													     >	                return (3, 9)  # Default fallback
													     >
													     >	        def get_version_compatible_deps(python_version: tuple) -> list:
													     >	            """Get dependencies compatible with the specific Python version."""
													     >	            major, minor = python_version
													     >	            
													     >	            # Base dependencies that work across versions
													     >	            base_deps = [
													     >	                "requests>=2.20",
													     >	                "filelock>=3.9",
													     >	                "aiohttp",
													     >	                "uv"
													     >	            ]
													     >	            
													     >	            # Add tomli for older Python versions
													     >	            if (major, minor) < (3, 11):
													     >	                base_deps.append("tomli")
													     >	            
													     >	            # Version-specific packaging and safety
													     >	            if (major, minor) == (3, 9):
													     >	                # Use older, compatible versions for Python 3.9
													     >	                base_deps.extend([
													     >	                    "packaging>=21.0,<23.0",  # Use older packaging for Python 3.9
													     >	                    "safety>=2.3.5,<3.0"     # Use safety 2.x for Python 3.9
													     >	                ])
													     >	            elif (major, minor) >= (3, 10):
													     >	                # Use newer versions for Python 3.10+
													     >	                base_deps.extend([
													     >	                    "packaging>=23.0",
													     >	                    "safety>=3.0"
													     >	                ])
													     >	            else:
													     >	                # Fallback for other versions
													     >	                base_deps.extend([
													     >	                    "packaging>=21.0",
													     >	                    "safety>=2.3.5"
													     >	                ])
													     >	                
													     >	            return base_deps
													     >
            print(_('   - Bootstrapping pip, setuptools, wheel...'))					     |	            print(_(' - Bootstrapping pip, setuptools, wheel...'))
													     >	                
            print(_('   ‚úÖ Pip bootstrap complete.'))							     |	            print(_(' ‚úÖ Pip bootstrap complete.'))
            # --- HYBRID STRATEGY ---									     |	            # Get Python version and compatible dependencies
            # Step 1: Install all of omnipkg's dependencies using the new interpreter's pip.		     |	            python_version = get_python_version(python_exe)
            # This allows pip to correctly resolve version-specific dependencies like 'tomli'.		     |	            print(_(' - Detected Python version: {}.{}').format(*python_version))
            core_deps = _get_core_dependencies()							     |	            
            if core_deps:										     |	            # Use version-compatible dependencies instead of pyproject.toml deps
                print(_('   - Installing omnipkg core dependencies...'))				     |	            compatible_deps = get_version_compatible_deps(python_version)
                # Let pip handle the markers correctly for the target python version			     |	            
                deps_install_cmd = [str(python_exe), '-m', 'pip', 'install', '--no-cache-dir'] + sorted(list |	            if compatible_deps:
                run_verbose(deps_install_cmd, 'Failed to install omnipkg dependencies.')		     |	                print(_(' - Installing version-compatible core dependencies...'))
                print(_('   ‚úÖ Core dependencies installed.'))						     |	                deps_install_cmd = [str(python_exe), '-m', 'pip', 'install', '--no-cache-dir'] + compatible_
													     >	                run_verbose(deps_install_cmd, 'Failed to install compatible dependencies.')
													     >	                print(_(' ‚úÖ Compatible core dependencies installed.'))
													     >	                
													     >	                # Install optional dependencies separately (they're more forgiving)
													     >	                print(_(' - Installing optional dependencies (non-critical)...'))
													     >	                optional_deps = ['tqdm', 'python-magic', 'redis>=5.0']
													     >	                for dep in optional_deps:
													     >	                    try:
													     >	                        opt_cmd = [str(python_exe), '-m', 'pip', 'install', '--no-cache-dir', dep]
													     >	                        run_verbose(opt_cmd, f'Failed to install {dep} (non-critical)')
													     >	                    except Exception as e:
													     >	                        print(_(' ‚ö†Ô∏è  Optional dependency {} failed, continuing...').format(dep))
            print(_('   - Installing omnipkg application layer...'))					     |	            print(_(' - Installing omnipkg application layer...'))
                print(_('     (Developer mode detected: performing editable install)'))			     |	                print(_(' (Developer mode detected: performing editable install)'))
                print('     (Standard mode detected: installing from PyPI)')				     |	                print(' (Standard mode detected: installing from PyPI)')
													     >	                
            print(_('   ‚úÖ Omnipkg bootstrapped successfully!'))					     |	            print(_(' ‚úÖ Omnipkg bootstrapped successfully!'))
													     |	            
        Creates a proper shell script executable that forces the use of the new Python interpreter.	     |	        Creates a proper shell script executable that is self-aware and always
													     >	        runs omnipkg using the CURRENTLY CONFIGURED Python interpreter.
        system = platform.system().lower()								     |	        config_file_path = self.config_path.resolve() # Get the absolute path to the config
        if system == 'windows':										     |
            script_content = f'@echo off\nREM This script was auto-generated by omnipkg to ensure the correc |	        # The content of the new, smarter shell script
													     >	        script_content = f"""#!/bin/bash
													     >	    # This script was auto-generated by omnipkg to be self-aware.
													     >	    # It reads the config file to find the currently active Python interpreter
													     >	    # and uses that to run the omnipkg CLI.
													     >
													     >	    # Find the config file path (this will be hardcoded)
													     >	    CONFIG_PATH="{config_file_path}"
													     >
													     >	    # Read the 'python_executable' value from the JSON config file.
													     >	    # We use a simple python command here to avoid complex shell parsing.
													     >	    # The `-I` flag is crucial to prevent the outer environment from interfering.
													     >	    ACTIVE_PYTHON=$(python -I -c "import json; f=open('$CONFIG_PATH'); d=json.load(f); print(d.get('environm
													     >
													     >	    # If we couldn't find the active python, fallback to the one this script was created with.
													     >	    if [ -z "$ACTIVE_PYTHON" ]; then
													     >	        ACTIVE_PYTHON="{new_python_exe.resolve()}"
													     >	    fi
													     >
													     >	    # Execute the omnipkg CLI using the determined active Python interpreter.
													     >	    exec "$ACTIVE_PYTHON" -m omnipkg.cli "$@"
													     >	    """
													     >
													     >	        # For Windows, create a batch file (simplified logic for brevity)
													     >	        if platform.system().lower() == 'windows':
													     >	            script_content = f'@echo off\nREM This script is a placeholder. A more robust PowerShell script 
        else:												     |
            script_content = f'#!/bin/bash\n# This script was auto-generated by omnipkg to ensure the correc <
        if system != 'windows':										     |	        
            omnipkg_exec_path.chmod(493)								     |	        # Make it executable
        print(_('   ‚úÖ New omnipkg executable created.'))						     |	        if platform.system().lower() != 'windows':
													     >	            os.chmod(omnipkg_exec_path, 0o755) # rwxr-xr-x
													     >	        
													     >	        print(_('   ‚úÖ New self-aware omnipkg executable created.'))
        print(_('   ‚ö†Ô∏è  This command does NOT uninstall any Python packages.'))				     |	        print(_('   ‚ö†Ô∏è  This command does NOT uninstall any Python packages unless they are corrupted due to 
													     >	        
													     >	    # In omnipkg/core.py, inside the omnipkg class
    def rebuild_knowledge_base(self, force: bool=False):						     |	    def rebuild_knowledge_base(self, force: bool = False):
        FIXED: Rebuilds the knowledge base by directly invoking the metadata gatherer			     |	        FIXED (Definitive): Rebuilds the knowledge base by spawning a new,
        in-process, avoiding subprocess argument limits and ensuring all discovered			     |	        clean subprocess using the CORRECT Python interpreter for the context.
        packages are processed correctly.								     |	        This completely prevents environment contamination.
													     >	            print("   ‚ùå Cannot rebuild KB: Cache connection failed.")
            from .package_meta_builder import omnipkgMetadataGatherer					     |	            # Get the path to the Python executable for the CURRENT context.
            gatherer = omnipkgMetadataGatherer(config=self.config, env_id=self.env_id, force_refresh=force)  |	            target_python_exe = self.config.get('python_executable', sys.executable)
            gatherer.cache_client = self.cache_client							     |	            print(f"   - Using interpreter: {target_python_exe}")
            gatherer.run()										     |
													     >	            # Get the path to the metadata builder script.
													     >	            builder_script_path = Path(__file__).parent / 'package_meta_builder.py'
													     >
													     >	            # Set an environment variable to pass the env_id to the subprocess.
													     >	            # This is more reliable than command-line arguments.
													     >	            env_for_subprocess = os.environ.copy()
													     >	            env_for_subprocess['OMNIPKG_ENV_ID_OVERRIDE'] = self.env_id
													     >
													     >	            # Construct the command to run the builder in a clean, isolated process.
													     >	            # The '-I' flag is CRITICAL to prevent any contamination.
													     >	            cmd = [
													     >	                target_python_exe,
													     >	                '-I', # Isolate the process from the parent environment's sys.path
													     >	                str(builder_script_path)
													     >	            ]
													     >	            if force:
													     >	                cmd.append('--force')
													     >
													     >	            print("   - Spawning isolated process to build knowledge base...")
													     >	            # Use Popen to stream the output live, so it feels integrated.
													     >	            process = subprocess.Popen(
													     >	                cmd,
													     >	                stdout=subprocess.PIPE,
													     >	                stderr=subprocess.STDOUT,
													     >	                text=True,
													     >	                encoding='utf-8',
													     >	                env=env_for_subprocess
													     >	            )
													     >
													     >	            # Stream the output from the subprocess
													     >	            for line in iter(process.stdout.readline, ''):
													     >	                print(f"   | {line.strip()}")
													     >	            
													     >	            return_code = process.wait()
													     >
													     >	            if return_code != 0:
													     >	                print("   ‚ùå Knowledge base rebuild subprocess failed.")
													     >	                return 1
													     >
													     >	            
        Checks for a flag file indicating a new interpreter needs its KB built.				     |	        Checks for an environment-specific flag file indicating a new
        If the current context matches a version in the flag, it runs the build.			     |	        interpreter needs its KB built.
        Returns True if a rebuild was run, False otherwise.						     <
        flag_file = self.config_manager.venv_path / '.omnipkg' / '.needs_kb_rebuild'			     |	        # --- THIS IS THE FIX ---
													     >	        # Look for the SAME environment-specific filename.
													     >	        flag_file = self.config_manager.venv_path / '.omnipkg' / f'.needs_kb_rebuild_{self.config_manager.en
													     >	        # --- END FIX ---
													     >
        # Determine the current Python context's version						     <
            return False # Cannot determine current version						     |	            return False
        												     |
        lock_file = self.config_manager.venv_path / '.omnipkg' / '.needs_kb_rebuild.lock'		     |	        lock_file = flag_file.with_suffix('.lock')
        with FileLock(lock_file):									     |	        with FileLock(str(lock_file)):
													     >	            # ... (rest of the function is correct and remains unchanged)
                    versions_to_rebuild = json.load(f)							     |	                    content = f.read()
													     >	                    if content:
													     >	                        versions_to_rebuild = json.loads(content)
                print("   Building its knowledge base now...")						     |	                print("   Building its knowledge base now (this may take a moment)...")
                											     |
                # Perform the rebuild for the current context						     <
                if rebuild_status == 0: # Check for success						     |	                if rebuild_status == 0:
                    # Remove the current version from the list and save back				     <
                        flag_file.unlink(missing_ok=True) # Clean up empty file				     |	                        flag_file.unlink(missing_ok=True)
                    return False # Rebuild failed, don't clear flag					     |	                    return False
													     >	            # --- THIS IS THE CORRECT FIX ---
													     >	            # Use '-E' to ignore PYTHON* env vars and '-s' to avoid adding the
													     >	            # user's site-packages. This correctly isolates the interpreter
													     >	            # to its OWN environment without breaking importlib.metadata.
													     >	            cmd = [self.config['python_executable'], '-E', '-s', '-c', script]
                [self.config['python_executable'], '-c', script],					     |	                cmd, capture_output=True, text=True, check=True, timeout=15
                capture_output=True, text=True, check=True, timeout=10					     <
													     >	            # --- END OF THE CORRECT FIX ---
            print(f"   ‚ö†Ô∏è  Could not perform live bulk package scan: {e}")				     |	            # Add more debug info on failure
													     >	            error_details = f"Error: {e}"
													     >	            if isinstance(e, subprocess.CalledProcessError):
													     >	                error_details += f"\nSTDERR:\n{e.stderr}"
													     >	            print(f"   ‚ö†Ô∏è  Could not perform live bulk package scan. {error_details}")
        Self-healing function with targeted and efficient approach.					     |	        Self-healing function that is fully aware of all active and bubbled
        Uses a pending rebuild flag and optimized bulk operations for maximum speed and reliability.	     |	        packages and will no longer incorrectly delete valid KB entries.
													     >	        (Definitive Version)
        rebuild_was_run = self._check_and_run_pending_rebuild()						     |	        if self._check_and_run_pending_rebuild():
        if rebuild_was_run:										     <
    													     |
        												     |
            self.cache_client										     |	            self.connect_cache()
    													     |	            if not self.cache_client:
													     >	                print("   ‚ùå Cannot perform sync: Cache connection failed.")
													     >	                return
													     >
													     >	        cached_packages_in_index = self.cache_client.smembers(index_key)
													     >
													     >	        # --- STEP 1: Establish the complete "Ground Truth" ---
        # Early exit if site-packages is empty								     |	        # Get all packages active in the main environment
        if not self.cache_client.exists(index_key):							     <
            if not any(Path(self.config['site_packages_path']).iterdir()):				     <
                print(_('   ‚úÖ Knowledge base is empty or no packages found to sync.'))			     <
                return											     <
    													     <
        # --- PERFORMANCE OPTIMIZATION ---								     <
        # Get all live versions in one fast command instead of in a loop.				     <
        												     |
        # Get cached active packages from Redis index							     |	        # Get ALL packages inside ALL bubbles using a deep scan
        cached_active_packages = self.cache_client.smembers(index_key)					     |	        packages_in_bubbles = set()
        												     <
        # --- TARGETED LOGIC ---									     <
        # Calculate differences for more efficient processing						     <
        to_add = set(live_active_versions.keys()) - cached_active_packages				     <
        to_remove = cached_active_packages - set(live_active_versions.keys())				     <
        to_check = cached_active_packages.intersection(set(live_active_versions.keys()))		     <
        												     <
        # Also include bubble versions in the comprehensive check					     <
        packages_with_bubbles = set()									     <
            for bubble_dir in self.multiversion_base.iterdir():						     |	            for dist_info_path in self.multiversion_base.rglob('*.dist-info'):
                if bubble_dir.is_dir():									     |	                if dist_info_path.is_dir():
                        dir_pkg_name, _version = bubble_dir.name.rsplit('-', 1)				     |	                        dist = importlib.metadata.Distribution.at(dist_info_path)
                        packages_with_bubbles.add(canonicalize_name(dir_pkg_name))			     |	                        packages_in_bubbles.add(canonicalize_name(dist.metadata['Name']))
                    except ValueError:									     |	                    except Exception:
                        continue									     |	                        continue # Skip malformed or unreadable dist-info
        												     |
        # Expand packages to check to include those with bubbles					     |	        # Combine them into a single source of truth
        to_check.update(packages_with_bubbles)								     |	        ground_truth_packages_on_disk = live_active_versions.keys() | packages_in_bubbles
													     >
													     >	        # --- STEP 2: Calculate the delta between the KB and reality ---
													     >
													     >	        stale_packages_in_kb = cached_packages_in_index - ground_truth_packages_on_disk
													     >	        missing_packages_in_kb = ground_truth_packages_on_disk - cached_packages_in_index
        if not to_add and not to_remove and not to_check:						     |	        packages_to_check = cached_packages_in_index.intersection(ground_truth_packages_on_disk)
            print(_('   ‚úÖ Knowledge base is empty or no packages found to sync.'))			     |	        version_discrepancies = 0
            return											     |
    													     |	        # --- STEP 3: Reconcile the differences ---
													     >
        # Handle new packages that need to be added to KB						     <
        if to_add:											     <
            print(_('   -> Found {} new packages to add to KB.').format(len(to_add)))			     <
            # Run targeted metadata builder for new packages						     <
            self._run_metadata_builder_for_delta({}, {name: live_active_versions[name] for name in to_add})  <
            fixed_count += len(to_add)									     <
        												     <
        # Use pipeline for bulk Redis operations							     <
            # Remove stale packages from KB								     |	            # A) Remove stale entries from the KB that no longer exist on disk
            if to_remove:										     |	            if stale_packages_in_kb:
                print(_('   -> Found {} stale packages to remove from KB.').format(len(to_remove)))	     |	                print(_('   -> Found {} stale packages to remove from KB.').format(len(stale_packages_in_kb)
                for pkg_name in to_remove:								     |	                for pkg_name in stale_packages_in_kb:
                    pipe.delete(main_key)								     |	                    # Delete all keys related to this package
													     >	                    keys_to_delete = self.cache_client.keys(f"{main_key}*")
													     >	                    if keys_to_delete:
													     >	                        pipe.delete(*keys_to_delete)
                fixed_count += len(to_remove)								     |	                fixed_count += len(stale_packages_in_kb)
            												     |
            # Check and update existing packages (including bubble versions)				     |	            # B) Check for version mismatches for active packages
            for pkg_name in to_check:									     |	            for pkg_name in packages_to_check:
                main_key = f'{self.redis_key_prefix}{pkg_name}'						     |	                if pkg_name in live_active_versions:
                											     |	                    real_active_version = live_active_versions[pkg_name]
                # Get real active version from pre-fetched dictionary					     |	                    cached_active_version = self.cache_client.hget(f'{self.redis_key_prefix}{pkg_name}', 'ac
                real_active_version = live_active_versions.get(pkg_name)				     |	                    if real_active_version != cached_active_version:
                											     |	                        pipe.hset(f'{self.redis_key_prefix}{pkg_name}', 'active_version', real_active_versio
                # Get real bubbled versions								     |	                        version_discrepancies += 1
                real_bubbled_versions = set()								     <
                if self.multiversion_base.exists():							     <
                    for bubble_dir in self.multiversion_base.iterdir():					     <
                        if not bubble_dir.is_dir(): 							     <
                            continue									     <
                        try:										     <
                            dir_pkg_name, version = bubble_dir.name.rsplit('-', 1)			     <
                            if canonicalize_name(dir_pkg_name) == pkg_name:				     <
                                real_bubbled_versions.add(version)					     <
                        except ValueError:								     <
                            continue									     <
                											     <
                # Get cached data for this package							     <
                cached_data = self.cache_client.hgetall(main_key)					     <
                cached_active_version = cached_data.get('active_version')				     <
                cached_bubbled_versions = {k.replace('bubble_version:', '') for k in cached_data if k.starts <
                											     <
                # Fix active version discrepancies							     <
                if real_active_version and real_active_version != cached_active_version:		     <
                    pipe.hset(main_key, 'active_version', real_active_version)				     <
                    fixed_count += 1									     <
                elif not real_active_version and cached_active_version:					     <
                    pipe.hdel(main_key, 'active_version')						     <
                    fixed_count += 1									     <
                											     <
                # Fix bubble version discrepancies							     <
                stale_bubbles = cached_bubbled_versions - real_bubbled_versions				     <
                for version in stale_bubbles:								     <
                    pipe.hdel(main_key, f'bubble_version:{version}')					     <
                    fixed_count += 1									     <
                											     <
                missing_bubbles = real_bubbled_versions - cached_bubbled_versions			     <
                for version in missing_bubbles:								     <
                    pipe.hset(main_key, f'bubble_version:{version}', 'true')				     <
                    fixed_count += 1									     <
        												     |
													     >	        fixed_count += version_discrepancies
													     >
													     >	        # C) Add missing packages to the KB (best handled by a targeted rebuild)
													     >	        if missing_packages_in_kb:
													     >	            print(_('   -> Found {} missing packages to add to KB.').format(len(missing_packages_in_kb)))
													     >	            # We create a dictionary of only the *newly added active* packages
													     >	            newly_active_to_add = {
													     >	                name: live_active_versions[name]
													     >	                for name in missing_packages_in_kb if name in live_active_versions
													     >	            }
													     >	            if newly_active_to_add:
													     >	                self._run_metadata_builder_for_delta({}, newly_active_to_add)
													     >	            fixed_count += len(missing_packages_in_kb)
													     >
													     >	        omnipkg_instance=self # Give it the power to call smart_install
                gatherer = omnipkgMetadataGatherer(config=self.config, env_id=self.env_id, force_refresh=Tru |	                gatherer = omnipkgMetadataGatherer(
													     >	                    config=self.config, 
													     >	                    env_id=self.env_id, 
													     >	                    force_refresh=True, 
													     >	                    omnipkg_instance=self 
													     >	                )
        Safely adopts a Python version by checking the registry, then trying to copy			     |	        Adopts a Python interpreter, then swaps the context and builds the knowledge base.
        from the local system, and finally falling back to download.					     |	        This is the robust, final version of this function.
        A rescan is forced after any successful filesystem change to ensure registration.		     <
        # First, check if it's already perfectly managed.						     |	        # Check if it's already managed.
        # Attempt to find it locally to copy.								     |	        # --- STEP 1: Get the interpreter onto the disk ---
													     >	        adoption_succeeded = False
        if not source_path_str:										     |	        if source_path_str:
            print(_('   - No local Python {} found. Falling back to download strategy.').format(version))    |	            # Try to copy from a local source first
            result = self._fallback_to_download(version)						     |	            print(f"   - Found local Python {version}. Attempting safe copy...")
            if result == 0:										     |	            source_exe_path = Path(source_path_str)
                print(_('üîß Forcing rescan to register the new interpreter...'))			     |	            try:
                self.rescan_interpreters()								     |	                # This block contains all the safety checks from your original code
            return result										     |	                source_root = Path(os.path.realpath(subprocess.check_output([str(source_exe_path), '-c', 'im
        												     |	                if self._is_same_or_child_path(source_root, self.config_manager.venv_path) or \
        source_exe_path = Path(source_path_str)								     |	                   not self._is_valid_python_installation(source_root, source_exe_path) or \
        try:												     |	                   self._estimate_directory_size(source_root) > 2 * 1024 * 1024 * 1024 or \
            cmd = [str(source_exe_path), '-c', 'import sys; print(sys.prefix)']				     |	                   self._is_system_critical_path(source_root):
            cmd_result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=10)	     |	                    print(_('   - ‚ö†Ô∏è  Safety checks failed for local copy. Falling back to download.'))
            source_root = Path(os.path.realpath(cmd_result.stdout.strip()))				     |	                else:
            current_venv_root = self.config_manager.venv_path.resolve()					     |	                    dest_root = self.config_manager.venv_path / '.omnipkg' / 'interpreters' / f'cpython-{ver
            												     |	                    if self._perform_safe_copy(source_root, dest_root, version) == 0:
            # Perform safety checks before attempting a copy						     |	                        adoption_succeeded = True
            if self._is_same_or_child_path(source_root, current_venv_root) or \				     |
            not self._is_valid_python_installation(source_root, source_exe_path) or \			     |	            except Exception as e:
            self._estimate_directory_size(source_root) > 2 * 1024 * 1024 * 1024 or \			     |	                print(_('   - ‚ùå Error during local copy attempt: {}. Falling back to download.').format(str
            self._is_system_critical_path(source_root):							     |	        
                print(_('   - ‚ö†Ô∏è  Safety checks failed for local copy. Falling back to download.'))	     |	        if not adoption_succeeded:
                result = self._fallback_to_download(version)						     |	            # If local copy was not possible or failed, download it.
                if result == 0:										     |	            print(_('   - No suitable local Python found. Falling back to download strategy.').format(versio
                    print(_('üîß Forcing rescan to register the downloaded interpreter...'))		     |	            if self._fallback_to_download(version) == 0:
                    self.rescan_interpreters()								     |	                adoption_succeeded = True
                return result										     |
            												     |	        # --- STEP 2: If adoption failed, stop here ---
            dest_root = self.config_manager.venv_path / '.omnipkg' / 'interpreters' / f'cpython-{version}'   |	        if not adoption_succeeded:
            if dest_root.exists():									     |	            print(f"‚ùå CRITICAL: Failed to get interpreter for Python {version} via copy or download.")
                print(_('   - ‚úÖ Adopted copy of Python {} already exists. Ensuring it is registered.').form |	            return 1
                self.rescan_interpreters()								     |	        
                return 0										     |	        # --- STEP 3: The Shared "Swap and Build" Orchestration ---
            												     |	        # This code will now run after ANY successful adoption.
            print(_('   - Starting safe copy operation...'))						     |	        self.rescan_interpreters()
            result = self._perform_safe_copy(source_root, dest_root, version)				     |
            												     |	        print("\n" + "="*60)
            if result == 0:										     |	        print(_("   ‚úÖ Interpreter for Python {} is now available.").format(version))
                print(_('üîß Forcing rescan to register the copied interpreter...'))			     |	        print(_("   Finalizing setup by switching the active context."))
                self.rescan_interpreters()								     |	        print("="*60)
            return result										     |
            												     |	        if self.switch_active_python(version) != 0:
        except Exception as e:										     |	            print(_("‚ùå CRITICAL: Failed to switch context to new Python {}.").format(version))
            print(_('   - ‚ùå An error occurred during the copy attempt: {}. Falling back to download.').form |	            return 1
            result = self._fallback_to_download(version)						     |
            if result == 0:										     |	        print(_("\nüéâ Successfully adopted and configured Python {}!").format(version))
                print(_('üîß Forcing rescan to register the downloaded interpreter...'))			     |	        print(_("   üí° The knowledge base will be built automatically the next time you run an omnipkg comma
                self.rescan_interpreters()								     |	        return 0
            return result										     <
    def smart_install(self, packages: List[str], dry_run: bool=False) -> int:				     |	    def smart_install(self, packages: List[str], dry_run: bool=False, force_reinstall: bool=False, target_di
													     >	            if force_reinstall:
													     >	                print("   - üõ°Ô∏è  Force reinstall triggered by auto-repair.")
            satisfaction_check = self._check_package_satisfaction([package_spec], strategy=install_strategy) |
            if satisfaction_check['all_satisfied']:							     |	            # If force_reinstall is True, we skip the satisfaction check entirely.
                print('‚úÖ Requirement already satisfied: {}'.format(package_spec))			     |	            if not force_reinstall:
                continue										     |	                satisfaction_check = self._check_package_satisfaction([package_spec], strategy=install_strat
													     >	                if satisfaction_check['all_satisfied']:
													     >	                    print('‚úÖ Requirement already satisfied: {}'.format(package_spec))
													     >	                    continue
            packages_to_install = satisfaction_check['needs_install']					     |	            packages_to_install = [package_spec]
            if not packages_to_install:									     <
                continue										     <
            												     |
            return_code = self._run_pip_install(packages_to_install)					     |	            return_code = self._run_pip_install(
													     >	                packages_to_install, 
													     >	                target_directory=target_directory, 
													     >	                force_reinstall=force_reinstall
													     >	            )
													     >	        
        print('\nüßπ Cleaning redundant bubbles...')							     |	        if not force_reinstall:
        final_active_packages = self.get_installed_packages(live=True)					     |	            print('\nüßπ Cleaning redundant bubbles...')
        cleaned_count = 0										     |	            final_active_packages = self.get_installed_packages(live=True)
        for pkg_name, active_version in final_active_packages.items():					     |	            cleaned_count = 0
            bubble_path = self.multiversion_base / f'{pkg_name}-{active_version}'			     |	            if not force_reinstall:
            if bubble_path.exists() and bubble_path.is_dir():						     |	                print('\nüßπ Cleaning redundant bubbles...')
                try:											     |	            for pkg_name, active_version in final_active_packages.items():
                    import shutil									     |	                bubble_path = self.multiversion_base / f'{pkg_name}-{active_version}'
                    shutil.rmtree(bubble_path)								     |	                if bubble_path.exists() and bubble_path.is_dir():
                    cleaned_count += 1									     |	                    try:
                    if hasattr(self, 'hook_manager'):							     |	                        shutil.rmtree(bubble_path)
                        self.hook_manager.remove_bubble_from_tracking(pkg_name, active_version)		     |	                        cleaned_count += 1
                except Exception as e:									     |	                        if hasattr(self, 'hook_manager'):
                    print(_('    ‚ùå Failed to remove bubble directory: {}').format(e))			     |	                            self.hook_manager.remove_bubble_from_tracking(pkg_name, active_version)
        												     |	                    except Exception as e:
        if cleaned_count > 0:										     |	                        print(_('    ‚ùå Failed to remove bubble directory: {}').format(e))
            print('    ‚úÖ Removed {} redundant bubbles'.format(cleaned_count))				     |	            
        												     |	            if cleaned_count > 0:
													     >	                print('    ‚úÖ Removed {} redundant bubbles'.format(cleaned_count))
													     >	                
													     >	    def _brute_force_package_cleanup(self, pkg_name: str, site_packages: Path):
													     >	        """
													     >	        Performs a manual, brute-force deletion of a corrupted package's files
													     >	        in a specific site-packages directory.
													     >	        """
													     >	        print(f"üßπ Performing brute-force cleanup of corrupted package '{pkg_name}' in {site_packages}...")
													     >	        try:
													     >	            c_name_dash = canonicalize_name(pkg_name)
													     >	            # Handle libraries that use underscores instead of dashes (e.g., flask_login)
													     >	            c_name_under = c_name_dash.replace('-', '_')
													     >	            
													     >	            # Delete library directories, checking for both dash and underscore variants
													     >	            for name_variant in {c_name_dash, c_name_under}:
													     >	                for path in site_packages.glob(f"{name_variant}"):
													     >	                    if path.is_dir():
													     >	                        print(f"   - Deleting library directory: {path}")
													     >	                        shutil.rmtree(path, ignore_errors=True)
													     >	            
													     >	            # Delete metadata directories
													     >	            for path in site_packages.glob(f"{c_name_dash}-*.dist-info"):
													     >	                 if path.is_dir():
													     >	                    print(f"   - Deleting metadata: {path}")
													     >	                    shutil.rmtree(path, ignore_errors=True)
													     >
													     >	            print("   - ‚úÖ Brute-force cleanup complete.")
													     >	            return True
													     >	        except Exception as e:
													     >	            print(f"   - ‚ùå Brute-force cleanup FAILED: {e}")
													     >	            return False
													     >
        print(_('üêç Switching active Python context to version {}...').format(version))			     |	        import time
													     >	        start_time = time.perf_counter_ns()
													     >	        
													     >	        # Get current Python version for the transition message
													     >	        current_config = self.config_manager.get('python_executable', '')
													     >	        current_version = None
													     >	        if current_config:
													     >	            # Extract version from current path
													     >	            for ver, path in self.interpreter_manager.list_available_interpreters().items():
													     >	                if str(path) == current_config:
													     >	                    current_version = ver
													     >	                    break
													     >	        
													     >	        # Check if we're already on the target version
													     >	        if current_version == version:
													     >	            print(_('‚úÖ Already using Python {}! No switch needed.').format(version))
													     >	            return 0
													     >	        
													     >	        transition_msg = f"from Python {current_version} " if current_version else ""
													     >	        print(_('üêç Switching active Python context {}to version {}...').format(transition_msg, version))
													     >	        
													     >	        
            print(_("   Run 'omnipkg list python' to see managed interpreters."))			     |	            print(_(" Run 'omnipkg list python' to see managed interpreters."))
            print(f"   If Python {version} is 'Discovered', first adopt it with: omnipkg python adopt {versi |	            print(f" If Python {version} is 'Discovered', first adopt it with: omnipkg python adopt {version
													     >	        
        print(_('   - Found managed interpreter at: {}').format(target_interpreter_str))		     |	        
													     >	        # Core switching logic with minimal output
        print(_('   - Updating configuration to new context...'))					     |	        
													     >	        # Update configuration
        print(_('   - ‚úÖ Configuration saved.'))							     |	        
        print(_('   - Updating default `python` symlinks...'))						     |	        # Update symlinks
            print(_('   - ‚ùå Failed to update symlinks: {}').format(e))					     |	            print(_('‚ùå Failed to update symlinks: {}').format(e))
													     >	            return 1
													     >	        
													     >	        # Calculate timing
													     >	        end_time = time.perf_counter_ns()
													     >	        swap_time_ns = end_time - start_time
													     >	        swap_time_us = swap_time_ns / 1000
													     >	        swap_time_ms = swap_time_us / 1000
													     >	        
													     >	        # Success message with timing - the differentiator!
        print('   The configuration has been updated. To activate the new interpreter')			     |	        print(f'‚ö° Python interpreter swap completed in {swap_time_us:,.1f} Œºs ({swap_time_ns:,} ns)')
        print(_('   in your shell, you MUST re-source your activate script:'))				     |	        
													     >	        # The classic "you MUST do this... just kidding!" 
													     >	        print(' The configuration has been updated. To activate the new interpreter')
													     >	        print(_(' in your shell, you MUST re-source your activate script:'))
													     >	        
													     |	     
													     |	    def _run_pip_install(self, packages: List[str], force_reinstall: bool=False, target_directory: Optional[
    def _run_pip_install(self, packages: List[str]) -> int:						     |	        """
        """Runs `pip install` with LIVE, STREAMING output."""						     |	        Runs `pip install` with LIVE, STREAMING output and automatic recovery
													     >	        from corrupted 'no RECORD file' errors. Can now target a specific directory.
													     >	        """
        try:												     |	        
            # Add '-u' for unbuffered output to force pip to talk in real-time.				     |	        cmd = [self.config['python_executable'], '-u', '-m', 'pip', 'install']
            cmd = [self.config['python_executable'], '-u', '-m', 'pip', 'install'] + packages		     |
													     >	        if force_reinstall:
													     >	            cmd.append('--upgrade')
            # Use Popen for live, line-by-line streaming.						     |	        if target_directory:
													     >	            print(f"   - Targeting installation to: {target_directory}")
													     >	            cmd.extend(['--target', str(target_directory)])
													     >	        
													     >	        cmd.extend(packages)
													     >	        
													     >	        try:
													     >	            # Use Popen for live streaming
                cmd,											     |	                cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True,
                stdout=subprocess.PIPE,									     |	                encoding='utf-8', errors='replace', bufsize=1, universal_newlines=True
                stderr=subprocess.STDOUT, # Redirect stderr to stdout					     <
                text=True,										     <
                encoding='utf-8',									     <
                errors='replace',									     <
                bufsize=1, # Line-buffered								     <
                universal_newlines=True									     <
            # Print each line of pip's output as it happens						     |	            stdout_lines, stderr_lines = [], []
            print() # Add a newline for better formatting						     <
            for line in iter(process.stdout.readline, ''):						     <
                # Print the raw line to preserve pip's own formatting (like progress bars)		     <
                print(line, end='') 									     <
            process.stdout.close()									     |	            # Read stdout and stderr in a non-blocking way
													     >	            for line in process.stdout:
													     >	                print(line, end='')
													     >	                stdout_lines.append(line)
													     >	            for line in process.stderr:
													     >	                print(line, end='', file=sys.stderr)
													     >	                stderr_lines.append(line)
													     >
													     >	            
													     >	            if return_code == 0:
													     >	                return 0 # Success!
													     >
													     >	            # --- HEALING LOGIC STARTS HERE ---
													     >	            full_stderr = "".join(stderr_lines)
													     >	            record_file_pattern = r"no RECORD file was found for ([\w\-]+)"
													     >	            match = re.search(record_file_pattern, full_stderr)
													     >
													     >	            if match:
													     >	                package_name = match.group(1)
													     >	                print("\n" + "="*60)
													     >	                print(f"üõ°Ô∏è  AUTO-RECOVERY: Detected corrupted package '{package_name}'.")
													     >	                
													     >	                # --- THIS IS THE FIX ---
													     >	                # Determine the correct location to clean up.
													     >	                cleanup_path = target_directory if target_directory else Path(self.config.get('site_packages
													     >	                
													     >	                if self._brute_force_package_cleanup(package_name, cleanup_path):
													     >	                # --- END OF THE FIX ---
													     >	                    print("   - Retrying installation on clean environment...")
													     >	                    # We can use the simpler subprocess.run here since it's a retry
													     >	                    retry_process = subprocess.run(cmd, capture_output=True, text=True)
													     >	                    if retry_process.returncode == 0:
													     >	                        print(retry_process.stdout)
													     >	                        print("   - ‚úÖ Recovery successful!")
													     >	                        return 0
													     >	                    else:
													     >	                        print("   - ‚ùå Recovery failed. Pip error after cleanup:")
													     >	                        print(retry_process.stderr)
													     >	                        return 1
													     >	                else:
													     >	                    return 1 # Brute force cleanup failed
													     >
													     >	            # If it was a different error, just return the original failure code
													     >
        try:												     |
            pip_version = version('pip')								     |	        # --- THIS IS THE FIX ---
            print(_('\nüîí Pip in Jail (main environment)'))						     |	        # Get live versions from the CORRECT, configured python interpreter via isolated subprocess
													     >	        # This prevents reading stale data from a parent conda environment.
													     >	        live_versions = self._get_all_active_versions_live()
													     >	        pip_version = live_versions.get('pip', 'not found')
													     >	        uv_version = live_versions.get('uv', 'not found')
													     >	        # --- END OF THE FIX ---
													     >
													     >	        print(_('\nüîí Pip in Jail (main environment)'))
													     >	        if pip_version != 'not found':
        except importlib.metadata.PackageNotFoundError:							     |	        else:
            print(_('\nüîí Pip in Jail (main environment)'))						     <
        try:												     |
            uv_version = version('uv')									     |	        print(_('üîí UV in Jail (main environment)'))
            print(_('üîí UV in Jail (main environment)'))						     |	        if uv_version != 'not found':
        except importlib.metadata.PackageNotFoundError:							     |	        else:
            print(_('üîí UV in Jail (main environment)'))						     <
													     >
        active_packages_count = len(list(site_packages.glob('*.dist-info')))				     |	        
													     >	        # Use the length of the live_versions dictionary for a more accurate count
													     >	        active_packages_count = len(live_versions)
													     >	        
													     >	        # ... (the rest of the function is unchanged)

=========================================================================================
END OF: omnipkg/core.py
=========================================================================================


=========================================================================================
START OF: omnipkg/package_meta_builder.py
=========================================================================================

													     >	from omnipkg.loader import omnipkgLoader
													     <
    def __init__(self, config: Dict, env_id: str, force_refresh: bool=False):				     |	    def __init__(self, config: Dict, env_id: str, force_refresh: bool = False, omnipkg_instance=None):
        # --- THE FIX ---										     |	        """
        # Rename cache_client to the generic cache_client. It will be set by the parent.		     |	        Initialize the metadata gatherer.
        self.cache_client = None									     <
        # --- END FIX ---										     <
        self.force_refresh = force_refresh								     |	        Args:
        self.security_report = {}									     |	            config: Configuration dictionary
        self.config = config										     |	            env_id: Environment ID
        self.env_id = os.environ.get('OMNIPKG_ENV_ID_OVERRIDE', env_id)					     |	            force_refresh: Whether to ignore caching
        self.package_path_registry = {}									     |	            omnipkg_instance: Optional reference to parent omnipkg instance
        if self.force_refresh:										     |	        """
            print(_('üü¢ --force flag detected. Caching will be ignored.'))				     |	        # Initialize cache client (will be set by parent if needed)
        if not HAS_TQDM:										     <
            print(_("‚ö†Ô∏è Install 'tqdm' for a better progress bar."))					     <
													     >	        
													     >	        # Store initialization parameters
													     |	        
        # --- CORE FIX: Prioritize the override ENV VAR to get the correct ID ---			     |	        # Prioritize the override ENV VAR to get the correct ID
        # This ensures this class instance uses the env_id passed from the parent.			     |	        # This ensures this class instance uses the env_id passed from the parent
        # --- END CORE FIX ---										     |	        
													     <
													     >	        
													     >	        # Store the reference to parent omnipkg instance
													     >	        self.omnipkg_instance = omnipkg_instance
													     >	        
													     >	        # Show status messages
													     >	        
        IMPROVED: Authoritatively discovers distributions with robust fallback mechanisms.		     |	        FIXED (Definitive): Authoritatively discovers distributions by EXPLICITLY
        Handles package name variations (dashes vs underscores) and case sensitivity issues.		     |	        scanning only the site-packages and bubble paths defined in the config for
        """												     |	        the current context. This prevents cross-environment contamination.
													     >	        """
													     >	        # --- THIS IS THE CRITICAL FIX ---
													     >	        # Get the correct paths for the context we are analyzing from the config.
													     >	        site_packages_path = Path(self.config.get('site_packages_path'))
													     >	        multiversion_base_path = Path(self.config.get('multiversion_base'))
													     >
													     >	        search_paths = []
													     >	        if site_packages_path.is_dir():
													     >	            search_paths.append(str(site_packages_path))
													     >	        if multiversion_base_path.is_dir():
													     >	            search_paths.append(str(multiversion_base_path))
													     >
													     >	        if not search_paths:
													     >	             print("‚ö†Ô∏è  Warning: No valid site-packages or bubble directories found in config. Discovery may 
													     >
													     >	                    
													     >	                    # NEW: Skip known sub-packages that are part of larger packages
													     >	                    if self._is_subpackage_component(name):
													     >	                        print(f'   -> Skipping {name} - detected as sub-component of larger package.')
													     >	                        continue
													     >	                    
													     >	        
													     >	    def _is_subpackage_component(self, package_name: str) -> bool:
													     >	        """
													     >	        Check if a package name is actually a sub-component of a larger package.
													     >	        """
													     >	        subpackage_patterns = {
													     >	            'tensorboard_data_server': 'tensorboard',
													     >	            'tensorboard_plugin_': 'tensorboard',  # Catches tensorboard_plugin_*
													     >	            # Add other known patterns here
													     >	        }
													     >	        
													     >	        for pattern, parent in subpackage_patterns.items():
													     >	            if package_name.startswith(pattern):
													     >	                # Verify the parent package exists
													     >	                try:
													     >	                    importlib.metadata.distribution(parent)
													     >	                    return True
													     >	                except importlib.metadata.PackageNotFoundError:
													     >	                    pass
													     >	        
													     >	        return False
        FIXED: Runs a security check using `safety`. Now properly handles the JSON			     |	        Runs a security check using a dedicated, isolated 'safety' tool bubble,
        output wrapped with deprecation warnings and correctly counts vulnerabilities.			     |	        created on-demand by the bubble_manager to guarantee isolation.
        scan_type = 'bulk' if len(packages) > 1 else 'targeted'						     |	        print(f'üõ°Ô∏è Performing security scan for {len(packages)} active package(s) using isolated tool...')
        if len(packages) == 0:										     <
            scan_type = 'targeted'									     <
        												     <
        print(f'üõ°Ô∏è Performing {scan_type} security scan for {len(packages)} active package(s)...')	     <
        if not packages:										     |	        if not packages or not self.omnipkg_instance:
            print(_(' - No active packages found to scan.'))						     |	            if not packages:
            self.security_report = {}									     |	                print(_(' - No active packages found to scan.'))
            return											     |	            else:
        												     |	                print(" ‚ö†Ô∏è Cannot run security scan: omnipkg_instance not available to builder.")
        python_exe = self.config.get('python_executable', sys.executable)				     <
        												     <
        # Check if safety is available									     <
        try:												     <
            subprocess.run([python_exe, '-m', 'safety', '--version'], check=True, capture_output=True, timeo <
        except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):		     <
            print(f" ‚ö†Ô∏è Warning: The 'safety' package is not installed for the active Python interpreter ({Pa <
            print(_(" üí° To enable this feature, run: '{} -m pip install safety'").format(python_exe))	     <
        												     |
        # Create requirements file									     |	        TOOL_SPEC = "safety==3.6.1"
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as reqs_fi |	        TOOL_NAME, TOOL_VERSION = TOOL_SPEC.split('==')
            reqs_file_path = reqs_file.name								     |
            for name, version in packages.items():							     <
                reqs_file.write(f'{name}=={version}\n')							     <
        												     <
            cmd = [python_exe, '-m', 'safety', 'check', '-r', reqs_file_path, '--json']			     |	            bubble_path = self.omnipkg_instance.multiversion_base / f"{TOOL_NAME}-{TOOL_VERSION}"
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120, encoding='utf-8')	     <
            if result.stdout:										     |	            # --- THIS IS THE CORRECT LOGIC ---
                raw_output = result.stdout.strip()							     |	            # We check for the physical directory. If it's not there, we build it.
                											     |	            if not bubble_path.is_dir():
                # Find JSON boundaries - safety wraps JSON with deprecation warnings			     |	                print(f" üí° First-time setup: Creating isolated bubble for '{TOOL_SPEC}' tool...")
                json_start = -1										     |	                # Use the dedicated bubble creation method, NOT smart_install
                json_end = -1										     |	                success = self.omnipkg_instance.bubble_manager.create_isolated_bubble(TOOL_NAME, TOOL_VERSIO
                											     |	                if not success:
                # Look for the opening brace/bracket							     |	                    print(f" ‚ùå Failed to create the tool bubble for {TOOL_SPEC}. Skipping scan.")
                brace_pos = raw_output.find('{')							     <
                bracket_pos = raw_output.find('[')							     <
                											     <
                if brace_pos != -1 and bracket_pos != -1:						     <
                    json_start = min(brace_pos, bracket_pos)						     <
                elif brace_pos != -1:									     <
                    json_start = brace_pos								     <
                elif bracket_pos != -1:									     <
                    json_start = bracket_pos								     <
                											     <
                if json_start != -1:									     <
                    # Find the matching closing brace/bracket						     <
                    opening_char = raw_output[json_start]						     <
                    closing_char = '}' if opening_char == '{' else ']'					     <
                    											     <
                    # Count nesting to find the proper end						     <
                    count = 0										     <
                    in_string = False									     <
                    escape_next = False									     <
                    											     <
                    for i in range(json_start, len(raw_output)):					     <
                        char = raw_output[i]								     <
                        										     <
                        if escape_next:									     <
                            escape_next = False								     <
                            continue									     <
                        										     <
                        if char == '\\':								     <
                            escape_next = True								     <
                            continue									     <
                        										     <
                        if char == '"' and not escape_next:						     <
                            in_string = not in_string							     <
                            continue									     <
                        										     <
                        if not in_string:								     <
                            if char == opening_char:							     <
                                count += 1								     <
                            elif char == closing_char:							     <
                                count -= 1								     <
                                if count == 0:								     <
                                    json_end = i + 1							     <
                                    break								     <
                    											     <
                    if json_end > json_start:								     <
                        json_content = raw_output[json_start:json_end]					     <
                        try:										     <
                            self.security_report = json.loads(json_content)				     <
                        except json.JSONDecodeError as e:						     <
                            print(f' ‚ö†Ô∏è Could not parse safety JSON output: {e}')			     <
                            self.security_report = {}							     <
                    else:										     <
                        # Fallback: try parsing from json_start to end and let json.loads find the boundary  <
                        try:										     <
                            # This will likely fail due to extra content, but worth trying		     <
                            self.security_report = json.loads(raw_output[json_start:])			     <
                        except json.JSONDecodeError:							     <
                            print(' ‚ö†Ô∏è Could not determine JSON boundaries')				     <
                            self.security_report = {}							     <
                else:											     <
                    print(' ‚ö†Ô∏è No JSON found in safety output')						     <
                											     |	                    return
                if result.stderr:									     |	                print(f" ‚úÖ Successfully created tool bubble.")
                    print(_(' ‚ö†Ô∏è Safety command produced warnings. Stderr: {}').format(result.stderr.strip()) |	            
            else:											     |	            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as reqs_file:
                self.security_report = {}								     |	                reqs_file_path = reqs_file.name
                if result.stderr:									     |	                for name, version in packages.items():
                    print(_(' ‚ö†Ô∏è Safety command failed. Stderr: {}').format(result.stderr.strip()))	     |	                    reqs_file.write(f'{name}=={version}\n')
                    											     |
													     >	            print(f" üåÄ Force-activating '{TOOL_SPEC}' context to run scan...")
													     >	            # The loader will now find the bubble that was just created.
													     >	            with omnipkgLoader(TOOL_SPEC, config=self.omnipkg_instance.config, force_activation=True):
													     >	                python_exe = self.config.get('python_executable', sys.executable)
													     >	                cmd = [python_exe, '-m', 'safety', 'check', '-r', reqs_file_path, '--json']
													     >	                result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)
													     >
													     >
													     >	            self.security_report = {}
													     >	            if result.stdout:
													     >	                try:
													     >	                    json_match = re.search(r'(\[.*\]|\{.*\})', result.stdout, re.DOTALL)
													     >	                    if json_match:
													     >	                        self.security_report = json.loads(json_match.group(1))
													     >	                except json.JSONDecodeError:
													     >	                    print(f' ‚ö†Ô∏è Could not parse safety JSON output.')
													     >	            
													     >	            if result.stderr and "error" in result.stderr.lower():
													     >	                 print(_(' ‚ö†Ô∏è Safety tool produced errors: {}').format(result.stderr.strip()))
													     >
            print(_(' ‚ö†Ô∏è An unexpected error occurred during the security scan: {}').format(e))		     |	            print(_(' ‚ö†Ô∏è An unexpected error occurred during the isolated security scan: {}').format(e))
            if os.path.exists(reqs_file_path):								     |	            if 'reqs_file_path' in locals() and os.path.exists(reqs_file_path):
        												     |
        # Count actual vulnerabilities from the parsed safety report					     |	        # Report the findings
        												     |	        if isinstance(self.security_report, list):
        if isinstance(self.security_report, dict):							     |	            issue_count = len(self.security_report)
            # Modern safety format has a 'vulnerabilities' key with the actual vulnerabilities		     |	        elif isinstance(self.security_report, dict) and 'vulnerabilities' in self.security_report:
            if 'vulnerabilities' in self.security_report:						     |	            issue_count = len(self.security_report['vulnerabilities'])
                vulnerabilities = self.security_report['vulnerabilities']				     <
                if isinstance(vulnerabilities, list):							     <
                    issue_count = len(vulnerabilities)							     <
            # Also check report_meta for vulnerabilities_found (more reliable)				     <
            elif 'report_meta' in self.security_report:							     <
                meta = self.security_report['report_meta']						     <
                if isinstance(meta, dict) and 'vulnerabilities_found' in meta:				     <
                    issue_count = int(meta['vulnerabilities_found'])					     <
            # Legacy format fallback									     <
            else:											     <
                for key, value in self.security_report.items():						     <
                    if isinstance(value, list) and key not in [						     <
                        'scanned_packages', 'scanned', 'scanned_full_path', 				     <
                        'target_languages', 'announcements', 'ignored_vulnerabilities'			     <
                    ]:											     <
                        issue_count += len(value)							     <
    def run(self, targeted_packages: Optional[List[str]]=None, newly_active_packages: Optional[Dict[str, str |	    def _discover_distributions_via_subprocess(self, search_paths: List[str]) -> List[importlib.metadata.Dis
        FIXED (v2): The main execution loop. It now safely handles corrupted				     |	        A hyper-isolated method to discover distributions using a clean subprocess.
        package metadata during the pre-scan phase, preventing crashes.					     |	        This is the definitive fix for environment contamination.
													     >	        print("   -> Using hyper-isolated subprocess for package discovery...")
													     >	        script = f"""
													     >	import sys
													     >	import json
													     >	import importlib.metadata
													     >	from pathlib import Path
													     >
													     >	# These paths are passed in from the correctly-configured builder instance
													     >	search_paths = {json.dumps(search_paths)}
													     >	dist_info_paths = []
													     >
													     >	try:
													     >	    # This call now happens in a pristine environment
													     >	    for dist in importlib.metadata.distributions(path=search_paths):
													     >	        # dist._path is the path to the .dist-info directory
													     >	        if dist._path and Path(dist._path).exists():
													     >	            dist_info_paths.append(str(dist._path))
													     >	    # Use set to ensure uniqueness before printing
													     >	    print(json.dumps(list(set(dist_info_paths))))
													     >	except Exception as e:
													     >	    # Print errors to stderr so they can be captured
													     >	    print(f"Discovery error: {{e}}", file=sys.stderr)
													     >	    sys.exit(1)
													     >	"""
													     >	        try:
													     >	            python_exe = self.config.get('python_executable', sys.executable)
													     >	            # Use -I for maximum isolation, preventing any .py files from the current
													     >	            # directory or PYTHONPATH from interfering.
													     >	            cmd = [python_exe, '-I', '-c', script]
													     >	            result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=120)
													     >	            dist_paths = json.loads(result.stdout)
													     >	            
													     >	            dists = []
													     >	            for path_str in dist_paths:
													     >	                try:
													     >	                    dists.append(importlib.metadata.Distribution.at(Path(path_str)))
													     >	                except Exception as e:
													     >	                    print(f"   -> Warning: Could not load distribution from discovered path '{path_str}': {e
													     >	                    continue
													     >	            print(f"   -> Hyper-isolation successful. Found {len(dists)} distributions.")
													     >	            return dists
													     >	        except (subprocess.CalledProcessError, json.JSONDecodeError, subprocess.TimeoutExpired) as e:
													     >	            error_output = e.stderr if hasattr(e, 'stderr') else str(e)
													     >	            print(f"‚ö†Ô∏è Subprocess-based discovery failed: {error_output}")
													     >	            print("‚ö†Ô∏è Falling back to direct (potentially contaminated) discovery method.")
													     >	            # Fallback to the original method only if the robust one fails
													     >	            return list(importlib.metadata.distributions(path=search_paths))
													     >
													     >	    def _discover_distributions(self, targeted_packages: Optional[List[str]]) -> List[importlib.metadata.Dis
													     >	        """
													     >	        Authoritatively discovers distributions by EXPLICITLY scanning only the
													     >	        site-packages and bubble paths from the config for the current context.
													     >	        Now uses a hyper-isolated subprocess to prevent cross-environment contamination.
													     >	        """
													     >	        if targeted_packages:
													     >	            # Targeted mode logic remains the same as it's less prone to contamination
													     >	            # (Code for targeted_packages is omitted for brevity but should be the same as your "flawed" fil
													     >	            print(f'üéØ Running in targeted mode for {len(targeted_packages)} package(s).')
													     >	            # ... (Your existing robust targeted discovery logic here) ...
													     >	            # For this fix, we assume the main problem is with full scans.
													     >	            # A simplified version for brevity:
													     >	            dists = []
													     >	            for spec in targeted_packages:
													     >	                try:
													     >	                    name, version = spec.split('==')
													     >	                    dist = importlib.metadata.distribution(name)
													     >	                    if dist.version == version:
													     >	                        dists.append(dist)
													     >	                except (importlib.metadata.PackageNotFoundError, ValueError):
													     >	                    continue
													     >	            return dists
													     >
													     >	        print('üîç Discovering all packages from file system (ground truth)...')
													     >	        site_packages_path = Path(self.config.get('site_packages_path'))
													     >	        multiversion_base_path = Path(self.config.get('multiversion_base'))
													     >
													     >	        search_paths = []
													     >	        if site_packages_path.is_dir():
													     >	            search_paths.append(str(site_packages_path))
													     >	        if multiversion_base_path.is_dir():
													     >	            # Add the base bubble directory and each individual bubble directory
													     >	            search_paths.append(str(multiversion_base_path))
													     >	            for p in multiversion_base_path.iterdir():
													     >	                if p.is_dir():
													     >	                    search_paths.append(str(p))
													     >
													     >	        if not search_paths:
													     >	             print("‚ö†Ô∏è  Warning: No valid site-packages or bubble directories found. Discovery may fail.")
													     >	             return []
													     >
													     >	        # *** THIS IS THE FIX ***
													     >	        # Always use the hyper-isolated subprocess for full scans.
													     >	        dists = self._discover_distributions_via_subprocess(search_paths)
													     >	        
													     >	        print(_('‚úÖ Discovery complete. Found {} total package versions to process.').format(len(dists)))
													     >	        return dists
													     >
													     >	    # (The rest of your omnipkgMetadataGatherer class methods remain the same)
													     >	    # ... (e.g., _is_bubbled, run, _process_package, etc.) ...
													     >	    def run(self, targeted_packages: Optional[List[str]]=None, newly_active_packages: Optional[Dict[str, str
													     >	        """
													     >	        The main execution loop. It now ensures the security
													     >	        tool bubble is created *before* discovering packages, guaranteeing a
													     >	        complete and accurate knowledge base build in a single pass.
													     >	        """
													     >	        # STEP 1: PREPARE THE ENVIRONMENT for security scan
													     >	        if not targeted_packages:
													     >	            print("üîß Preparing environment for full scan...")
													     >	            try:
													     >	                # Perform a preliminary, non-isolated discovery just to get active packages for the scan
													     >	                active_dists = [dist for dist in importlib.metadata.distributions() if not self._is_bubbled(
													     >	                active_packages_for_scan = {
													     >	                    canonicalize_name(dist.metadata['Name']): dist.version
													     >	                    for dist in active_dists if 'Name' in dist.metadata
													     >	                }
													     >	                self._perform_security_scan(active_packages_for_scan)
													     >	            except Exception as e:
													     >	                print(f"‚ö†Ô∏è Could not perform pre-scan for security tool setup: {e}")
													     >	                self.security_report = {}
													     >	        
													     >	        # STEP 2: DISCOVER THE COMPLETE, ISOLATED GROUND TRUTH.
													     >
													     >	        # STEP 3: RUN SECURITY SCAN (for targeted mode)
            newly_active_packages_to_scan = {}								     |	            packages_to_scan = {
            for dist in distributions_to_process:							     |	                canonicalize_name(dist.metadata['Name']): dist.version
                if not self._is_bubbled(dist):								     |	                for dist in distributions_to_process if 'Name' in dist.metadata
                    raw_name = dist.metadata.get('Name')						     |	            }
                    if raw_name:									     |	            self._perform_security_scan(packages_to_scan)
                        newly_active_packages_to_scan[canonicalize_name(raw_name)] = dist.version	     |
            self._perform_security_scan(newly_active_packages_to_scan)					     <
        else:												     <
            all_active_packages_to_scan = {}								     <
            for dist in distributions_to_process:							     <
                if not self._is_bubbled(dist):								     <
                    raw_name = dist.metadata.get('Name')						     <
                    if raw_name:									     <
                        name = canonicalize_name(raw_name)						     <
                        all_active_packages_to_scan[name] = dist.version				     <
                    else:										     <
                        print(_("\n‚ö†Ô∏è  WARNING: Skipping corrupted package found at '{}'.").format(dist._path <
                        print(_("    This package's metadata is missing a name. This is often caused by an") <
                        print(_("    interrupted 'pip install'. Please manually delete this directory."))    <
            self._perform_security_scan(all_active_packages_to_scan)					     <
													     >	            
													     >	            
													     >	        processed_packages = set()
													     >	        
													     >	        distributions_to_process.sort(key=lambda d: self._is_bubbled(d))
													     >	        
            if self._process_package(dist):								     |	            try:
                updated_count += 1									     |	                raw_name = dist.metadata.get('Name')
        print(_('\nüéâ Metadata building complete! Updated {} package(s).').format(updated_count))	     |	                if not raw_name: continue
													     >	                
													     >	                name = canonicalize_name(raw_name)
													     >	                version = dist.version
													     >	                pkg_id = (name, version)
													     >
													     >	                if pkg_id in processed_packages:
													     >	                    continue
													     >	                
													     >	                if self._process_package(dist):
													     >	                    updated_count += 1
													     >	                    processed_packages.add(pkg_id)
													     >	            except Exception:
													     >	                continue
													     >
													     >	    def _is_known_subcomponent(self, dist_info_path: Path) -> bool:
													     >	        """Check if this dist-info belongs to a sub-component that shouldn't be treated independently."""
													     >	        name = dist_info_path.name
													     >	        
													     >	        # Known sub-components that are part of larger packages
													     >	        subcomponent_patterns = [
													     >	            'tensorboard_data_server-',
													     >	            'tensorboard_plugin_',
													     >	        ]
													     >	        
													     >	        for pattern in subcomponent_patterns:
													     >	            if name.startswith(pattern):
													     >	                return True
													     >	        
													     >	        return False
        FIXED: Processes a single, definitive Distribution object and now gracefully			     |	        Processes a single distribution and automatically triggers repair
        handles corrupted packages that might lack a name or other critical metadata.			     |	        for corrupted packages, respecting their original location.
                print(_("\n‚ö†Ô∏è  WARNING: Skipping corrupted package found at '{}'.").format(dist._path))	     |	                # NEW: Check if this is a known sub-component before flagging as corrupted
                print(_("    This package's metadata is missing a name. This often happens when an"))	     |	                if self._is_known_subcomponent(dist._path):
                print(_('    install is interrupted. To fix, please manually delete this directory and'))    |	                    print(f"   -> Skipping {dist._path.name} - known sub-component of larger package.")
                print(_('    re-run your command.'))							     |	                    return False  # Skip processing this distribution
                return False										     |	                
													     >	                print(_("\n‚ö†Ô∏è CORRUPTION DETECTED: Package at '{}' is missing a name.").format(dist._path))
													     >	                # ... rest of existing corruption handling code
													     >	                
													     >	                if self.omnipkg_instance:
													     >	                    print("   - üõ°Ô∏è  Attempting auto-repair...")
													     >	                    match = re.match(r"([\w\.\-]+)-([\w\.\+a-z0-9]+)\.dist-info", dist._path.name)
													     >	                    if match:
													     >	                        name, version = match.groups()
													     >	                        package_to_reinstall = f"{name}=={version}"
													     >	                        
													     >	                        target_dir = None
													     >	                        cleanup_path = Path(self.omnipkg_instance.config.get('site_packages_path'))
													     >	                        multiversion_base = self.omnipkg_instance.config.get('multiversion_base', '/dev/null
													     >	                        
													     >	                        if str(dist._path).startswith(multiversion_base):
													     >	                            # The corruption is inside a bubble.
													     >	                            # The target for reinstall is the bubble root.
													     >	                            # The target for cleanup is ALSO the bubble root.
													     >	                            target_dir = dist._path.parent
													     >	                            cleanup_path = dist._path.parent
													     >	                            print(f"   - Corruption is inside a bubble. Repair will target: {target_dir}")
													     >	                        
													     >	                        # --- THIS IS THE FIX ---
													     >	                        # Pass the correct cleanup_path to the cleanup function
													     >	                        self.omnipkg_instance._brute_force_package_cleanup(name, cleanup_path)
													     >	                        # --- END OF THE FIX ---
													     >	                        
													     >	                        print(f"   - üöÄ Re-installing '{package_to_reinstall}' to heal the environment...")
													     >	                        self.omnipkg_instance.smart_install(
													     >	                            [package_to_reinstall], 
													     >	                            force_reinstall=True, 
													     >	                            target_directory=target_dir
													     >	                        )
													     >	                    else:
													     >	                        print("   - ‚ùå Auto-repair failed: Could not parse package name from path.")
													     >	                else:
													     >	                    print(_("    To fix, please manually delete this directory and re-run your command."))
													     >
													     >	                return False # We did not process this specific (corrupted) item
													     >
													     >	            
                        # --- START MODIFIED CODE ---							     |
            is_active = not self._is_bubbled(dist) # Determine status from path				     |	            is_active = not self._is_bubbled(dist)
            # --- END MODIFIED CODE ---									     <
													     >	# At the very end of omnipkg/package_meta_builder.py
													     >
    import hashlib # Ensure hashlib is imported here							     |	    import hashlib
    print(_('üöÄ Starting omnipkg Metadata Builder v11 (Multi-Version Complete Edition)...'))		     |	    
													     >	    print(_('üöÄ Starting omnipkg Metadata Builder (Standalone Subprocess Mode)...'))
													     >	    
													     >	        # This logic is now inside the __main__ block, making the script runnable.
        # --- CORE FIX: Determine the one true env_id, prioritizing the override ---			     |	        # The env_id is now passed reliably via an environment variable.
        env_id_from_os = os.environ.get('OMNIPKG_ENV_ID_OVERRIDE')					     |	        env_id = os.environ.get('OMNIPKG_ENV_ID_OVERRIDE')
        if env_id_from_os:										     |	        if not env_id:
            env_id = env_id_from_os									     |	            raise ValueError("OMNIPKG_ENV_ID_OVERRIDE not set in subprocess environment.")
            print(f"   (Inherited environment ID: {env_id})")						     <
        else:												     <
            # This is the fallback for when the script is run directly					     <
            # We must find the venv root the same way ConfigManager does.				     <
            current_dir = Path(sys.executable).resolve().parent						     <
            venv_path = Path(sys.prefix) # fallback							     <
            while current_dir != current_dir.parent:							     <
                if (current_dir / 'pyvenv.cfg').exists():						     <
                    venv_path = current_dir								     <
                    break										     <
                current_dir = current_dir.parent							     <
            env_id = hashlib.md5(str(venv_path.resolve()).encode()).hexdigest()[:8]			     <
            print(f"   (Calculated environment ID: {env_id})")						     <
        # --- END CORE FIX ---										     <
													     >	        print(f"   (Running in context of environment ID: {env_id})")
    except (FileNotFoundError, KeyError) as e:								     |
        print(f'‚ùå CRITICAL: Could not load omnipkg configuration for this environment (ID: {env_id}). Error |	    except (FileNotFoundError, KeyError, ValueError) as e:
													     >	        print(f'‚ùå CRITICAL: Could not load omnipkg configuration for this environment. Error: {e}. Aborting
    gatherer = omnipkgMetadataGatherer(config=config, env_id=env_id, force_refresh='--force' in sys.argv)    |	    # We create the omnipkg instance here to pass to the gatherer,
													     >	    # solving the "omnipkg_instance not available" bug permanently.
													     >	    from omnipkg.core import ConfigManager, omnipkg as OmnipkgCore
													     >	    config_manager = ConfigManager()
													     >	    omnipkg_instance = OmnipkgCore(config_manager)
													     >
													     >	    gatherer = omnipkgMetadataGatherer(
													     >	        config=config,
													     >	        env_id=env_id,
													     >	        force_refresh='--force' in sys.argv,
													     >	        omnipkg_instance=omnipkg_instance
													     >	    )
        if gatherer.connect_cache():									     |	        # The connection logic needs to exist in the Gatherer for this to work
            targeted_packages = [arg for arg in sys.argv[1:] if not arg.startswith('--')]		     |	        if not gatherer.cache_client:
            if targeted_packages:									     |	            # A simplified connection method for the builder
                gatherer.run(targeted_packages=targeted_packages)					     |	            try:
            else:											     |	                redis_host = gatherer.config.get('redis_host', 'localhost')
                gatherer.run()										     |	                redis_port = gatherer.config.get('redis_port', 6379)
            print(_('\nüéâ Metadata building complete!'))						     |	                gatherer.cache_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
        else:												     |	                gatherer.cache_client.ping()
            print(_('‚ùå Failed to connect to Redis. Aborting.'))					     |	            except Exception:
            sys.exit(1)											     |	                print('‚ùå Builder subprocess failed to connect to Redis. Aborting.')
													     >	                sys.exit(1)
													     >
													     >	        targeted_packages = [arg for arg in sys.argv[1:] if not arg.startswith('--')]
													     >	        gatherer.run(targeted_packages=targeted_packages if targeted_packages else None)
													     >	        print(_('\nüéâ Subprocess metadata building complete!'))
													     >	        sys.exit(0)
													     >	        
        print(_('\n‚ùå An unexpected error occurred during metadata build: {}').format(e))		     |	        print(_('\n‚ùå An unexpected error occurred during metadata build subprocess: {}').format(e))

=========================================================================================
END OF: omnipkg/package_meta_builder.py
=========================================================================================


=========================================================================================
START OF: omnipkg/cli.py
=========================================================================================

    """Main application entry point with pre-flight version check."""					     |	    """Main application entry point."""

=========================================================================================
END OF: omnipkg/cli.py
=========================================================================================


=========================================================================================
START OF: omnipkg/commands/run.py
=========================================================================================

													     <
													     >	import threading
													     >	from omnipkg.common_utils import ensure_python_or_relaunch 
def analyze_runtime_failure_and_heal(stderr: str, cmd_args: list, config_manager: ConfigManager):	     |	def analyze_runtime_failure_and_heal(stderr: str, cmd_args: list, config_manager: ConfigManager, summarize: 
													     >	    # --- THIS IS THE FINAL PIECE YOU NEED TO ADD ---
													     >	    # This pattern catches the specific ImportError and extracts the module name.
													     >	    import_error_pattern = r"ImportError: cannot import name '.*?' from '([\w\.]+)'"
													     >	    match = re.search(import_error_pattern, stderr)
													     >	    if match:
													     >	        module_name = match.group(1)  # e.g., 'google.protobuf'
													     >	        package_name = convert_module_to_package_name(module_name) # e.g., 'protobuf'
													     >
													     >	        if summarize:
													     >	            print(f"üí• Initial run failed due to: Incompatible dependency for '{package_name}'.")
													     >
													     >	        print(f"\nüîç In-place dependency error detected. Auto-healing by reinstalling '{package_name}'...")
													     >	        # This is the correct action: heal the main environment directly.
													     >	        return heal_with_missing_package(package_name, Path(cmd_args[0]), cmd_args[1:], config_manager)
													     >	    # --- END OF THE FIX ---
    attempt, and catching both explicit and implicit failures for auto-healing.				     |	    attempt, and catching failures for auto-healing with summarized output.
													     >	    # This is the correct bootstrap for the 'run' command
													     >	    if os.environ.get('OMNIPKG_RELAUNCHED') != '1':
													     >	        ensure_python_or_relaunch("3.11")
													     >	    sync_context_to_runtime()
													     >
													     |	        
    print(_(" syncing omnipkg context...")); sync_context_to_runtime(); print(_("‚úÖ Context synchronized.")) <
													     <
													     >	    initial_cmd = ['uv', 'run', '--no-project', '--python', python_exe, '--', 'python', '-u'] + cmd_args
    print(_("üöÄ Attempting to run script with uv, forcing use of current environment..."))		     |	    # --- Spinner Logic ---
    initial_cmd = ['uv', 'run', '--no-project', '--python', python_exe, '--'] + cmd_args		     |	    stop_spinner = threading.Event()
													     |	    def spinner():
    start_time_ns = time.perf_counter_ns()								     |	        chars = "-\\|/"
													     |	        i = 0
    process = subprocess.Popen(										     |	        while not stop_spinner.is_set():
        initial_cmd,											     |	            print(f"\rüöÄ Running script in background, please wait... {chars[i % len(chars)]}", end="", flus
        stdout=subprocess.PIPE,										     |	            time.sleep(0.1)
        stderr=subprocess.STDOUT,									     |	            i += 1
        text=True,											     |	        print("\r" + " " * 50 + "\r", end="", flush=True)
        encoding='utf-8',										     |
        cwd=Path.cwd(),											     |	    spinner_thread = threading.Thread(target=spinner)
        bufsize=1,											     |	    spinner_thread.daemon = True
        universal_newlines=True										     |	    spinner_thread.start()
    )													     <
    output_lines = []											     |	    process = None
        # Stream output live for immediate user feedback						     |	        # Run the command silently, capturing output
        for line in iter(process.stdout.readline, ''):							     |	        start_time_ns = time.perf_counter_ns()
            print(line, end='', flush=True)								     |	        process = subprocess.run(
            output_lines.append(line)									     |	            initial_cmd,
    except KeyboardInterrupt:										     |	            capture_output=True,
        print("\nüõë Process interrupted by user")							     |	            text=True,
        process.terminate()										     |	            encoding='utf-8',
        process.wait()											     |	            cwd=Path.cwd()
        return 130											     |	        )
													     |	        end_time_ns = time.perf_counter_ns()
    return_code = process.wait()									     |	        _initial_run_time_ns = end_time_ns - start_time_ns
    end_time_ns = time.perf_counter_ns()								     |	    finally:
    full_output = "".join(output_lines)									     |	        # Always stop the spinner
    _initial_run_time_ns = end_time_ns - start_time_ns							     |	        stop_spinner.set()
													     |	        spinner_thread.join()
    # --- Comprehensive Failure Detection ---								     <
    # Define patterns that indicate a healable error even if the exit code is 0.			     <
    healable_error_patterns = [										     <
        r"A module that was compiled using NumPy 1\.x cannot be run in[\s\S]*?NumPy 2\.0",		     <
        r"numpy\.dtype size changed, may indicate binary incompatibility"				     <
    ]													     <
    # Check if a healable error exists in the output, regardless of exit code.				     |	    full_output = process.stdout + process.stderr
    has_healable_error = any(re.search(pattern, full_output, re.MULTILINE) for pattern in healable_error_pat <
    if return_code == 0 and not has_healable_error:							     |	    if process.returncode == 0:
													     >	        print(full_output, end='')
    # If we are here, either the script failed outright or had a detectable issue.			     |	    print("‚è±Ô∏è  UV run failed in: {:.3f} ms ({:,} ns)".format(_initial_run_time_ns / 1_000_000, _initial_run_t
    if return_code == 0:										     |	    exit_code, heal_stats = analyze_runtime_failure_and_heal(full_output, cmd_args, config_manager, summariz
        print("\nüîç UV succeeded but detected healable errors in output...")				     <
    else:												     <
        print("‚è±Ô∏è  UV run failed in: {:.3f} ms ({:,} ns)".format(_initial_run_time_ns / 1_000_000, _initial_r <
    # Trigger the healing process.									     |	    if exit_code != 0 and not heal_stats:
    exit_code, heal_stats = analyze_runtime_failure_and_heal(full_output, cmd_args, config_manager)	     |	        print("\n" + "="*70)
													     >	        print(" B A S E   E R R O R   O U T P U T")
													     >	        print("="*70)
													     >	        print(full_output)
													     >	        print("="*70)
													     >	    
        else:												     |	        # By removing the 'else', we silently and correctly ignore paths that don't exist.
            print(f"   ‚ùå site-packages path doesn't exist: {sp_path}")					     |	        
    													     <
        												     |	                    
            print(f"      ‚ö†Ô∏è  Already in sys.path")							     |	            print(f"      ‚ÑπÔ∏è  Already in sys.path") # Changed icon
            print(f"      ‚ö†Ô∏è  Already in sys.path or None")						     |	            print(f"      ‚ÑπÔ∏è  Already in sys.path or None") # Changed icon
        												     |	          
            with omnipkgLoader(package_spec, config=config) as loader:					     |	            # --- FIX PART 1: Pass quiet=False to the loader to ensure debug output is on for the test ---
													     >	            with omnipkgLoader(package_spec, config=config, quiet=False) as loader:
                print(f"\\nüîç DEBUG: sys.path after bubble activation ({{len(sys.path)}} entries):")	     |	                # --- FIX PART 2: Change 'self.quiet' to 'loader.quiet' ---
                for i, path in enumerate(sys.path[:8]):  # Show first 8 entries				     |	                if not loader.quiet:
                    exists = "‚úÖ" if os.path.exists(path) else "‚ùå"					     |	                    print(f"\\nüîç DEBUG: sys.path after bubble activation ({{len(sys.path)}} entries):")
                    print(f"   [{{i}}] {{exists}} {{path}}")						     |	                    for i, path in enumerate(sys.path[:8]):  # Show first 8 entries
                if len(sys.path) > 8:									     |	                        path_obj = Path(path)
                    print(f"   ... and {{len(sys.path) - 8}} more entries")				     |	                        # This check is more robust and correctly handles the .zip file
                											     |	                        status = "‚úÖ" if path_obj.is_dir() or path_obj.is_file() else "‚ùå"
                # Test critical imports inside bubble							     |	                        print(f"   [{{i}}] {{status}} {{path}}")
                print(f"\\nüß™ Testing imports inside bubble...")					     |	                    
                for module_name in ['packaging', 'filelock', 'toml']:					     |	                    # Test critical imports inside bubble
                    try:										     |	                    print(f"\\nüß™ Testing imports inside bubble...")
                        module = __import__(module_name)						     |	                    for module_name in ['packaging', 'filelock', 'toml']:
                        print(f"      ‚úÖ {{module_name}}: {{module.__file__}}")				     |	                        try:
                    except ImportError as e:								     |	                            module = __import__(module_name)
                        print(f"      ‚ùå {{module_name}} failed: {{e}}")				     |	                            print(f"      ‚úÖ {{module_name}}: {{module.__file__}}")
                        # Try to find it in current sys.path						     |	                        except ImportError:
                        for i, path in enumerate(sys.path):						     |	                            # This now reports the missing module as informational, not a failure.
                            if os.path.exists(os.path.join(path, module_name)):				     |	                            print(f"      ‚ÑπÔ∏è Optional module '{{module_name}}' not found in bubble.")
                                print(f"         Found {{module_name}} in sys.path[{{i}}]: {{path}}")	     |	                            # Try to find it in current sys.path
                                break									     |	                            for i, path in enumerate(sys.path):
                            elif os.path.exists(os.path.join(path, f"{{module_name}}.py")):		     |	                                if os.path.exists(os.path.join(path, module_name)):
                                print(f"         Found {{module_name}}.py in sys.path[{{i}}]: {{path}}")     |	                                    print(f"         Found {{module_name}} in sys.path[{{i}}]: {{path}}")
                                break									     |	                                    break
													     >	                                elif os.path.exists(os.path.join(path, f"{{module_name}}.py")):
													     >	                                    print(f"         Found {{module_name}}.py in sys.path[{{i}}]: {{path}}")
													     >	                                    break

=========================================================================================
END OF: omnipkg/commands/run.py
=========================================================================================


=========================================================================================
START OF: omnipkg/loader.py
=========================================================================================

    def __init__(self, package_spec: str=None, config: dict=None):					     |	    def __init__(self, package_spec: str=None, config: dict=None, quiet: bool=False, force_activation: bool=
        Initializes the loader. If used as a context manager, package_spec is required.			     |	        Initializes the loader. Config is highly recommended for robust path discovery.
        Config is highly recommended for robust path discovery.						     <
        if self.config and 'multiversion_base' in self.config and ('site_packages_path' in self.config):     |	        self.quiet = quiet
													     >	        self.force_activation = force_activation
													     >
													     >	        # --- THIS IS THE FIX ---
													     >	        # Prioritize the provided config. If not available, use a more reliable
													     >	        # method to find the true site-packages of the RUNNING interpreter,
													     >	        # which is better than a simple sys.prefix fallback.
													     >	        if self.config and 'multiversion_base' in self.config and 'site_packages_path' in self.config:
            print(_('‚ö†Ô∏è [omnipkg loader] Config not provided or incomplete. Attempting auto-detection of path |	            if not self.quiet:
													     >	                print(_('‚ÑπÔ∏è [omnipkg loader] Config not provided. Auto-detecting paths for current interprete
													     >	                # This is a more robust way to find the actual site-packages
													     >	                import site
                print(_('‚ö†Ô∏è [omnipkg loader] Could not auto-detect site-packages path reliably. Falling back  |	                # This fallback is now more of an edge case
													     >	                if not self.quiet:
													     >	                    print(_('‚ö†Ô∏è [omnipkg loader] Could not auto-detect site-packages path reliably. Using sys
        if found_deps:											     |	        # Summarized logging
													     >	        if found_deps and not self.quiet:
													     >	            # THIS BLOCK IS NOW CORRECTLY INDENTED
                # Fallback: add the original site-packages to sys.path in a strategic position		     |	                # On failure, we just proceed to the fallback.
													     >	                # Do not print an error here as it is not a fatal condition.
													     >	                # The fallback is adding the original site-packages to sys.path.
        if linked_count > 0 or preserved_count > 0:							     |	        if (linked_count > 0 or preserved_count > 0) and not self.quiet:
        if cleaned_count > 0:										     |	        if cleaned_count > 0 and not self.quiet:
													     >	    # In omnipkg/loader.py
													     >
            raise ValueError("omnipkgLoader must be instantiated with a package_spec (e.g., 'pkg==ver') when |	            raise ValueError("omnipkgLoader must be instantiated with a package_spec when used as a context 
        print(_('\nüåÄ omnipkg loader: Activating {}...').format(self._current_package_spec))		     |	        if not self.quiet:
													     >	            print(_('\nüåÄ omnipkg loader: Activating {}...').format(self._current_package_spec))
            pkg_name_normalized = pkg_name.lower().replace('-', '_')					     <
        # Check if system version already matches							     |	        if not self.force_activation:
        try:												     |	            try:
            current_system_version = get_version(pkg_name)						     |	                current_system_version = get_version(pkg_name)
            if current_system_version == requested_version:						     |	                if current_system_version == requested_version:
                self._activation_end_time = time.perf_counter_ns()					     |	                    self._activation_end_time = time.perf_counter_ns()
                self._total_activation_time_ns = self._activation_end_time - self._activation_start_time     |	                    self._total_activation_time_ns = self._activation_end_time - self._activation_start_time
                print(_(' ‚úÖ System version already matches requested version ({}). No bubble activation nee |	                    if not self.quiet:
                print(_(' ‚è±Ô∏è  Activation time: {:.3f} Œºs ({:,} ns)').format(self._total_activation_time_ns /  |	                        print(_(' ‚úÖ System version already matches requested version ({}). No bubble activa
                self._activated_bubble_path = None							     |	                    self._activation_successful = True # Mark as "successful" to allow clean exit
                self._activation_successful = True							     |	                    return self
                return self										     |	            except PackageNotFoundError:
        except PackageNotFoundError:									     |	                pass
            pass											     |
        except Exception as e:										     |	        bubble_dir_name = f'{pkg_name}-{requested_version}' # Use original name for dir
            print(_('‚ö†Ô∏è [omnipkg loader] Error checking system version for {}: {}. Proceeding with bubble sea <
            pass											     <
        												     <
        # Find and activate bubble									     <
        bubble_dir_name = f'{pkg_name_normalized}-{requested_version}'					     <
            raise RuntimeError(_("Bubble not found for {} at {}. Please ensure it's installed via 'omnipkg i |	            raise RuntimeError(_("Bubble not found for {} at {}.").format(self._current_package_spec, bubble
            # Clean up target package modules and cloak main installation				     <
            # Update PATH if bubble has bin directory							     <
                print(_(' ‚öôÔ∏è Added to PATH: {}').format(bubble_bin_path))				     |	                if not self.quiet: print(_(' ‚öôÔ∏è Added to PATH: {}').format(bubble_bin_path))
            # Rebuild sys.path with bubble taking precedence						     |	            # --- THIS IS THE DEFINITIVE FIX ---
            sys.path.clear()										     |	            # Rebuild sys.path cleanly, filtering out contaminated paths.
            sys.path.insert(0, bubble_path_str)								     |	            new_sys_path = []
            # Add back original paths, skipping the original site-packages				     |	            # 1. Add the bubble path. Highest priority.
													     >	            new_sys_path.append(bubble_path_str)
													     >
													     >	            # Get the version string of the CURRENTLY EXECUTING interpreter (e.g., 'python3.9')
													     >	            current_interpreter_version_str = f'python{sys.version_info.major}.{sys.version_info.minor}'
													     >
													     >	            # 2. Add back original paths, but ONLY if they are not from a different Python's stdlib.
													     >	                path_str = str(p)
													     >	                # The contamination check: is '/lib/pythonX.Y' in the path, and is it the WRONG X.Y?
													     >	                if '/lib/python' in path_str and current_interpreter_version_str not in path_str:
													     >	                    # This is a contaminated path from a different Python stdlib. SKIP IT.
													     >	                    continue
													     >	                
													     >	                # Also skip the original site-packages, which is handled later.
                if p not in sys.path:									     |	                    
                    sys.path.append(p)									     |	                if p not in new_sys_path:
													     >	                    new_sys_path.append(p)
            # Ensure omnipkg dependencies are accessible for subprocess operations			     |	            sys.path.clear()
													     >	            sys.path.extend(new_sys_path)
													     >	            # --- END OF THE DEFINITIVE FIX ---
													     >
            # Add the original site-packages as a fallback (but lower priority)				     <
            if str(self.site_packages_root) not in sys.path:						     <
                sys.path.append(str(self.site_packages_root))						     <
            												     <
            												     <
            												     |	            self._activation_successful = True
            print(_(' ‚úÖ Activated bubble: {}').format(bubble_path_str))				     |
            print(_(' üîß sys.path[0]: {}').format(sys.path[0]))						     |	            if not self.quiet:
            print(_(' üîó Ensured omnipkg dependency access for subprocess support'))			     |	                print(_(' ‚úÖ Activated bubble: {}').format(bubble_path_str))
            print(_(' ‚è±Ô∏è  Activation time: {:.3f} Œºs ({:,} ns)').format(self._total_activation_time_ns / 1000 |	                print(_(' üîß sys.path[0]: {}').format(sys.path[0]))
            												     |	                print(_(' üîó Ensured omnipkg dependency access for subprocess support'))
            # Show bubble info										     |	                print(_(' ‚è±Ô∏è  Activation time: {:.3f} Œºs ({:,} ns)').format(self._total_activation_time_ns / 
            manifest_path = bubble_path / '.omnipkg_manifest.json'					     |	                manifest_path = bubble_path / '.omnipkg_manifest.json'
            if manifest_path.exists():									     |	                if manifest_path.exists():
                with open(manifest_path, 'r') as f:							     |	                    with open(manifest_path, 'r') as f: manifest = json.load(f)
                    manifest = json.load(f)								     <
            												     |
            self._activation_successful = True								     <
            print(_(' ‚ùå Activation failed: {}').format(str(e)))					     |	            if not self.quiet: print(_(' ‚ùå Activation failed: {}').format(str(e)))

=========================================================================================
END OF: omnipkg/loader.py
=========================================================================================


=========================================================================================
START OF: tests/test_rich_switching.py
=========================================================================================

        result = subprocess.run([python_exe, temp_script_path], capture_output=True, text=True, timeout=60)  |	        # --- THIS IS THE FIX for the TEST HARNESS ---
													     >	        # Add the '-I' flag to run the subprocess in isolated mode. This is the
													     >	        # correct way to write a test that involves multiple Python versions.
													     >	        # It ensures the test is valid and not suffering from environment leaks.
													     >	        result = subprocess.run(
													     >	            [python_exe, '-I', temp_script_path],
													     >	            capture_output=True,
													     >	            text=True,
													     >	            timeout=60
													     >	        )
													     >	        # --- END OF THE FIX ---
													     >

=========================================================================================
END OF: tests/test_rich_switching.py
=========================================================================================

